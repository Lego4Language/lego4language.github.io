<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.4.1" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.1">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.1" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '6.4.1',
    sidebar: {"position":"left","width":"14.5em","display":"hide","offset":12,"b2t":false,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  





  <meta name="description" content="2019年3月3.17　链表(t) + 词嵌入(t)【行】  《数据结构与算法：Python语言描述》线性表（链表）章节的代码实现和理解；  【悟】  集中时间和精力做最重要的事； 代码的规范性影响了代码的阅读和使用 ; —&amp;gt; 编码规范 学习如何系统的设计编码非常重要，像今天这种编码就比较随意，解决问题的效率较低，从明天开始学习《剑指Offer》解决上述问题，在学习这本书的时候，结合自己">
<meta property="og:type" content="website">
<meta property="og:title" content="《 2019 足迹 》">
<meta property="og:url" content="https://lego4language.github.io/Menu-Log/2019/index.html">
<meta property="og:site_name" content="喂，这世界！">
<meta property="og:description" content="2019年3月3.17　链表(t) + 词嵌入(t)【行】  《数据结构与算法：Python语言描述》线性表（链表）章节的代码实现和理解；  【悟】  集中时间和精力做最重要的事； 代码的规范性影响了代码的阅读和使用 ; —&amp;gt; 编码规范 学习如何系统的设计编码非常重要，像今天这种编码就比较随意，解决问题的效率较低，从明天开始学习《剑指Offer》解决上述问题，在学习这本书的时候，结合自己">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://lego4language.github.io/DIY/picture/【日志】头图.png">
<meta property="og:image" content="https://lego4language.github.io/DIY/picture/【日志】03.png">
<meta property="og:image" content="https://lego4language.github.io/DIY/picture/【日志】04.png">
<meta property="og:image" content="https://lego4language.github.io/DIY/picture/【日志】07.png">
<meta property="og:image" content="https://lego4language.github.io/DIY/picture/【日志】几个NLP常用库的对比.png">
<meta property="og:image" content="https://lego4language.github.io/DIY/picture/【日志】02.png">
<meta property="og:image" content="https://lego4language.github.io/DIY/picture/【日志】01.png">
<meta property="og:image" content="https://lego4language.github.io/DIY/picture/【日志】08.png">
<meta property="og:image" content="https://lego4language.github.io/DIY/picture/【日志】06.png">
<meta property="og:image" content="https://lego4language.github.io/DIY/picture/【日志】05.png">
<meta property="og:image" content="https://lego4language.github.io/DIY/picture/【待做】面试的流程.png">
<meta property="og:updated_time" content="2020-11-23T12:47:07.918Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《 2019 足迹 》">
<meta name="twitter:description" content="2019年3月3.17　链表(t) + 词嵌入(t)【行】  《数据结构与算法：Python语言描述》线性表（链表）章节的代码实现和理解；  【悟】  集中时间和精力做最重要的事； 代码的规范性影响了代码的阅读和使用 ; —&amp;gt; 编码规范 学习如何系统的设计编码非常重要，像今天这种编码就比较随意，解决问题的效率较低，从明天开始学习《剑指Offer》解决上述问题，在学习这本书的时候，结合自己">
<meta name="twitter:image" content="https://lego4language.github.io/DIY/picture/【日志】头图.png">






  <link rel="canonical" href="https://lego4language.github.io/Menu-Log/2019/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>《 2019 足迹 》 | 喂，这世界！</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  
  
    
  

  <!-- 彩带特效 -->
  
  
  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo" style="margin-left: 0px">
    
      <div class="site-meta-headline">
        <a href="https://lego4language.github.io/">
          <img class="custom-logo-image" src="/images/logo.png" alt="喂，这世界！"/>
        </a>
      </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <p id="nav-toggle-up">—— 物语先生</p>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-博客">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />博客</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-行业">
    <a href="/Menu-Industry/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-globe"></i> <br />行业</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-规划">
    <a href="/Menu-Plan/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-crosshairs"></i> <br />规划</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-问题">
    <a href="/Menu-Question/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />问题</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-关于">
    <a href="/Menu-About/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-github"></i> <br />关于</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-分类">
    <a href="/Menu-Category/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />分类</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-资源">
    <a href="/Menu-Source/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-skype"></i> <br />资源</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-足迹 menu-item-active">
    <a href="/Menu-Log/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-paw"></i> <br />足迹</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-趣文">
    <a href="/Menu-InterestingPaper/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-cube"></i> <br />趣文</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-建站">
    <a href="/Menu-Web/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-power-off"></i> <br />建站</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-标签">
    <a href="/Menu-Tag/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-地图">
    <a href="/Menu-Map/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-bug"></i> <br />地图</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-总结">
    <a href="/Menu-Summary/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-hourglass"></i> <br />总结</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-生活">
    <a href="/Menu-Life/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />生活</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />搜索</a>
        </li>
      
    </ul>
  

  
    

    
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
      
      
    
    

  


  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



                  <!-- 这是添加的submenu目录 2019-09-25-->
                  <link href="/css/submenu/submenu.css" rel="stylesheet" type="text/css" />
                  <ul id="nav">
                      <li id="first"><a class="hsubs" href="#">《 学科 》</a>
                          <ul class="first">
                              <li><a href="/SubMenu-Subject-Vision/"><p id="smallest">视觉</p></a></li>
                              <li><a href="/SubMenu-Subject-CS/"><p id="smallest">计算机</p></a></li>
                              <li><a href="/SubMenu-Subject-Speech/"><p id="smallest">语音</p></a></li>
                              <li><a href="/SubMenu-Subject-Cognitive/"><p id="smallest">认知学</p></a></li>
                              <li><a href="/SubMenu-Subject-TextMining/"><p id="smallest">文本挖掘</p></a></li>
                              <li><a href="/SubMenu-Subject-Philosophy/"><p id="smallest">哲学</p></a></li>
                              <li><a href="/SubMenu-Subject-InformationRetrieval/"><p id="smallest"></p>信息检索</a></li>
                              <li><a href="/SubMenu-Subject-Psychology/"><p id="smallest">心理学</p></a></li>
                              <li><a href="/SubMenu-Subject-RecommendedSystem/"><p id="smallest">推荐系统</p></a></li>
                              <li><a href="/SubMenu-Subject-Linguistic/"><p id="smallest">语言学</p></a></li>
                          </ul>
                      </li>
                      <li id="second"><a class="hsubs" href="#">《 框架 》</a>
                          <ul class="second">
                              <li><a href="/SubMenu-Frame-MachineLearning/"><p id="smallest">机器学习</p></a></li>
                              <li><a href="/SubMenu-Frame-DeepLearning/"><p id="smallest">深度学习</p></a></li>
                              <li><a href="/SubMenu-Frame-GNN/"><p id="smallest">图神经网络</p></a></li>
                              <li><a href="/SubMenu-Frame-TransferLearning/"><p id="smallest">迁移学习</p></a></li>
                              <li><a href="/SubMenu-Frame-ReinforcementLearning/"><p id="smallest">强化学习</p></a></li>
                              <li><a href="/SubMenu-Frame-GAN/"><p id="smallest">对抗生成网络</p></a></li>
                              <li><a href="/SubMenu-Frame-MetaLearning/"><p id="smallest">元学习</p></a></li>
                              <li><a href="/SubMenu-Frame-WeeklySupervisedLearning/"><p id="smallest">弱监督学习</p></a></li>
                              <li><a href="/SubMenu-Frame-SelfSupervisedLearning/"><p id="smallest">自监督学习</p></a></li>
                              <li><a href="/#"><p id="smallest">无监督学习</p></a></li>
                              <li><a href="/SubMenu-Frame-ContrastiveLearning/"><p id="smallest">对比学习</p></a></li>
                          </ul>
                      </li>
                      <li id="third"><a class="hsubs" href="#">《 分支 》</a>
                          <ul class="third">
                              <li><a href="/SubMenu-Subdomain-FeatureEngineering/"><p id="smallest">特征工程</p></a></li>
                              <li><a href="/SubMenu-Subdomain-TextProcessing/"><p id="smallest">文本预处理</p></a></li>
                              <li><a href="/SubMenu-Subdomain-RepresentationLearning"><p id="smallest">表示学习</p></a></li>
                              <li><a href="/SubMenu-Subdomain-PreTraining/"><p id="smallest">预训练</p></a></li>
                              <li><a href="/SubMenu-Subdomain-LanguageModel/"><p id="smallest">语言模型</p></a></li>
                              <li><a href="/SubMenu-Subdomain-Segmentation/"><p id="smallest">分词</p></a></li>
                              <li><a href="/SubMenu-Subdomain-POS/"><p id="smallest">词性标注</p></a></li> 
                              <li><a href="/SubMenu-Subdomain-NER/"><p id="smallest">命名实体识别</p></a></li>
                              <li><a href="/SubMenu-Subdomain-TextErrorCorrection"><p id="smallest">文本纠错</p></a></li>
                              <li><a href="#"><p id="smallest">语句改写</p></a></li>
                              <li><a href="#"><p id="smallest">语种识别</p></a></li>
                              <li><a href="#"><p id="smallest">关键词提取</p></a></li>
                              <li><a href="#"><p id="smallest">敏感词过滤</p></a></li>
                              <li><a href="/SubMenu-Subdomain-DependencyParse/"><p id="smallest">依存分析</p></a></li>
                              <li><a href="/SubMenu-Subdomain-ConstituencyParse/"><p id="smallest">成分解析</p></a></li>
                              <li><a href="/SubMenu-Subdomain-SentimentAnalysis/"><p id="smallest">情感分析</p></a></li>
                              <li><a href="/SubMenu-Subdomain-IntentRecognition/"><p id="smallest">意图识别</p></a></li>
                              <li><a href="/SubMenu-Subdomain-SemanticParsing/"><p id="smallest">语义分析</p></a></li>
                              <li><a href="/SubMenu-Subdomain-CoReferenceResolution/"><p id="smallest">指代消解</p></a></li>
                              <li><a href="/SubMenu-Subdomain-TextClassification/"><p id="smallest">文本分类</p></a></li>
                              <li><a href="#"><p id="smallest">文本聚类</p></a></li>
                              <li><a href="/SubMenu-Subdomain-TextMatch/"><p id="smallest">文本匹配</p></a></li>
                              <li><a href="/SubMenu-Subdomain-NLG/"><p id="smallest">文本生成</p></a></li>
                              <li><a href="/SubMenu-Subdomain-Summarization/"><p id="smallest">文本摘要</p></a></li>
                              <li><a href="#"><p id="smallest">文本蕴含</p></a></li> 
                              <li><a href="/SubMenu-Subdomain-IE/"><p id="smallest">信息抽取</p></a></li>       
                              <li><a href="#"><p id="smallest">风格转换</p></a></li>  
                              <li><a href="#"><p id="smallest">观点挖掘</p></a></li>
                              <li><a href="/SubMenu-Subdomain-MultitaskLearning/"><p id="smallest">多任务学习</p></a></li>
                              <li><a href="/SubMenu-Subdomain-ReadingComprehension"><p id="smallest">阅读理解</p></a></li>
                              <li><a href="#"><p id="smallest">繁简转化</p></a></li>
                              <li><a href="/SubMenu-Subdomain-NLI/"><p id="smallest">自然语言推理</p></a></li>
                              <li><a href="/SubMenu-Subdomain-QA/"><p id="smallest">问答系统</p></a></li>
                              <li><a href="/SubMenu-Subdomain-TaskBot/"><p id="smallest">任务型对话</p></a></li>  
                              <li><a href="/SubMenu-Subdomain-ChatBot/"><p id="smallest">闲聊系统</p></a></li>  
                              <li><a href="/SubMenu-Subdomain-ML/"><p id="smallest">机器翻译</p></a></li>
                          </ul>
                      </li>
                      <li id="forth"><a class="hsubs" href="#">《 主题 》</i></a>
                          <ul class="forth">
                            <li><a href="/SubMenu-Theme-Knowledge/"><p id="smallest">知识 + </p></a></li>
                            <li><a href="/SubMenu-Theme-Multimodals/"><p id="smallest">多模态</p></a></li>
                            <li><a href="/SubMenu-Theme-Explainable/"><p id="smallest">可解释性</p></a></li>
                            <li><a href="/SubMenu-Theme-ZeroShot/"><p id="smallest">零资源</p></a></li>
                            <li><a href="/SubMenu-Theme-FewShot/"><p id="smallest">低资源</p></a></li>
                            <li><a href="/SubMenu-Theme-RichSource/"><p id="smallest">高资源</p></a></li>
                            <li><a href="/SubMenu-Theme-Cause/"><p id="smallest">因果逻辑</p></a></li>
                            <li><a href="/SubMenu-Theme-ModelCompression/"><p id="smallest">模型压缩与加速</p></a></li>
                            <li><a href="/SubMenu-Theme-DataAugmentation/"><p id="smallest">数据增强</p></a></li>
                            <li><a href="/SubMenu-Theme-SourceandEvaluation/"><p id="smallest">资源和评估</p></a></li>
                          </ul>
                      </li>
                      <li id="fifth"><a class="hsubs" href="#">《 数学 》</a>
                          <ul class="fifth">
                            <li><a href="/SubMenu-Subject-Math/"><p id="smallest">《 总览 》</p></a></li>
                            <li><a href="/SubMenu-Subject-Math-Analysis/"><p id="smallest">Ψ 分析</p></a></li>
                            <li><a href="/SubMenu-Subject-Math-Algebra/"><p id="smallest">Φ 代数</p></a></li>
                            <li><a href="/SubMenu-Subject-Math-Geometry-Topology/"><p id="smallest">Ω 几何与拓扑</p></a></li>
                            <li><a href="/SubMenu-Subject-Math-Number-Theory/"><p id="smallest">ξ 数论</p></a></li>
                            <li><a href="/SubMenu-Subject-Math-Probability-Statistic/"><p id="smallest">Σ 概率与统计</p></a></li>
                            <li><a href="/SubMenu-Subject-Math-Dynamic-System/"><p id="smallest">Θ 动力系统</p></a></li>
                            <li><a href="/SubMenu-Subject-Math-Fundamental/"><p id="smallest">ζ 基础数学</p></a></li>
                            <li><a href="/SubMenu-Subject-Math-Physics/"><p id="smallest">ε 数学物理</p></a></li>
                            <li><a href="/SubMenu-Subject-Math-Differential-Equations/"><p id="smallest">δ 微分方程</p></a></li>
                            <li><a href="/SubMenu-Subject-Math-Discrete-Math/"><p id="smallest">Γ 离散数学</p></a></li>
                            <li><a href="/SubMenu-Subject-Math-Operations/"><p id="smallest">β 运筹学</p></a></li>
                            <li><a href="/SubMenu-Subject-Math-History/"><p id="smallest">α 科普及历史</p></a></li>
                          </ul>
                      </li>
                      <div id="lavalamp"><img src="/images/lavalamp.png"></div>
                  </ul>
                  <!--submenu目录结束-->
      </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
    
    <div class="post-block page">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">《 2019 足迹 》</h1>

<div class="post-meta">
  
  


  
  
  <ul class="breadcrumb">
    
      
      
        
          <li><a href="/Menu-Log/">MENU-LOG</a></li>
          
            
          
        
      
    
      
      
        
          <li>2019</li>
        
      
    
      
      
    
  </ul>


</div>

</header>

      
      
      
      <div class="post-body">
        
        
          <center><div style="width: 100%"><img src="/DIY/picture/【日志】头图.png" alt="link"></div></center>

<h2 id="2019年3月"><a href="#2019年3月" class="headerlink" title="2019年3月"></a>2019年3月</h2><h3 id="3-17-链表-t-词嵌入-t"><a href="#3-17-链表-t-词嵌入-t" class="headerlink" title="3.17　链表(t) + 词嵌入(t)"></a>3.17　链表(t) + 词嵌入(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《数据结构与算法：Python语言描述》线性表（链表）章节的代码实现和理解；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>集中时间和精力做最重要的事；</li>
<li>代码的规范性影响了代码的阅读和使用 ; —&gt; <b><a href="https://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/contents/" target="_blank" rel="noopener">编码规范</a></b></li>
<li>学习如何系统的设计编码非常重要，像今天这种编码就比较随意，解决问题的效率较低，从明天开始学习《剑指Offer》解决上述问题，在学习这本书的时候，结合自己刚刚学的数据结构或者算法进行配合学习，顺带提高自己对该知识点的理解和掌握；</li>
<li>通过练习发现，练习目的之一是更好的理解和掌握数据结构的特性，比如链表这个数据结构中，最重要的特性就是找到头指针或者尾指针就能够牵出所有的 元素，而不用想像顺序表一样，一眼就能看到整个表的全貌;</li>
<li>设计编程方案是<b>应该结合数据结构或者算法本身的特点</b>，比如在学习列表反转的时候，链表实现的反转可以通过表头插入和删除操作时间复杂度为O(1)来进行启发式的思考，而顺序表可以利用其查找元素为O(1)的时间复杂度来启发性地思考解决方案；</li>
<li>在学习书中的例子时，最好自己先思考解决方案甚至实现，这样才能理解某些书中方案的巧妙之处从而自己学习提高；</li>
<li>选择代码理解时尽量选择高手的代码或者官方的源码，这样既学习了算法，也能学习编程技巧，杂七杂八的代码看多了影响自己的编程感觉；</li>
<li>写代码可以用jupyter notebook，调试代码可以用pycharm或者vscode；</li>
</ul>
<p><b id="log">【待】</b></p>
<ul>
<li>学习词嵌入已经好几天了，但是原理始终没有理解透彻，明天先从理解CBOW的两种实现原理开始学习，然后开始使用词嵌入现成的软件，了解都干了些什么，能用在哪些应用上，然后再来看自己是否需要自己实现，这个模式以后在学习其他算法和模型的时候也可以借鉴参考；<br><br><br><br><hr>



</li>
</ul>
<h3 id="3-18-链表-c-词嵌入-t"><a href="#3-18-链表-c-词嵌入-t" class="headerlink" title="3.18　链表(c) + 词嵌入(t)"></a>3.18　链表(c) + 词嵌入(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《word2vec中的数学》40页全部学习完毕；</li>
<li>《剑指Offer》第一章 面试的流程</li>
<li>《数据结构与算法：Python语言描述》线性表 一章的总复习；</li>
</ul>
<p><b id="log">【悟】</b><br> 在刚开始学DeepLearning.ai中的词嵌入的时候，对词嵌入有了一个初步的了解，相对全面地理解了词嵌入中的一些概念，但是还是有很多地方流程讲得并不是很清楚，比如：</p>
<ul>
<li>提到的词嵌入矩阵其实是作为一个类似于隐藏层的存在；</li>
<li>提到的语言模型用神经网络来实现，其实就是神经语言模型，但是它对神经语言模型的具体实现流程并没有讲明白；</li>
<li>提到的skip-gram没有细致的数学原理，导致具体如何训练并不清楚，也就说这个时候只知道词向量的一些模糊的流程；</li>
<li>CBOW并没有仔细讲；</li>
<li>负采样的流程讲了，但是并没有讲原理是怎样的，也就是说为什么这样做是有效的；</li>
<li>词向量除偏并没有具体的推导，只讲了大概的流程；</li>
</ul>
<p>这些问题的存在导致了对词向量的理解并不是很深刻甚至有时候是<b>错误的理解</b>，只了解大概的样子，但是已经有了相对系统的了解，知道了具体应该有哪些知识点。<br> 课程看了之后也查了一些网络上的资料，但是基本上都是讲个大概，并没有细致的讲解和推理，直到遇到《word2vec中的数学》之后，情况才开始好起来，这本资料仔细地推导了词向量训练的数学原理，词向量中skip-gram、CBOW两种方法和hierachical softmax、negative sampling两种优化方法的关系，神经语言模型和两种方法之间的关系、负采样的原理等等，但是，这个资料中也有讲得不明白甚至错误的地方，比如：</p>
<ul>
<li>预备知识逻辑回归的损失函数讲解并不清楚；</li>
<li>语言模型的计算采用n-gram的具体原因是某个长句出现的概率很可能为0，而不是它的统计有多复杂；</li>
</ul>
<p>整体来说，之所以在学习《word2vec中的数学》时<b>比较顺畅地理解内容</b>，很大原因在于之前学过Andrew Ng的词嵌入课程，在遇到某些讲得不是很清楚的概念时，能够借鉴到Andrew讲的内容，同时，《word2vec中的数学》又对Andrew课程进行了更为细致的推导和细节扩充，这个学习过程给了我如下的启发：</p>
<ul>
<li>选择专业研究员（比如Andrew Ng)所设计的资料很大程度上能够正确、准确地帮助我们了解概念，网络上的资料虽然有一定的借鉴和启发作用，但是其理解的正确性和深度都有待考证，所以<b>学习资料的选择</b>很重要，在看网络博文的时候，我们可以从博文长度、有无目录、作者是否有大量其他文章看出作者的专业程度从而判断资料的可靠性；</li>
<li>选择参考资料的时候尽量选择讲解得<b>比较系统的</b>，这种资料一方面可以体现作者对这部分知识点了解透彻，有时候能学习大不同的角度来理解算法模型，另一方面可以从一些细节方面对算法进行补充； </li>
<li>再专业的资料也有它 <b>没有涉及到</b> 或者 <b>没有讲明白</b> 甚至 <b>讲错了的</b> 地方，所以需要多多阅读相关方面的其他资料大胆质疑，同时也可以从其它角度理解模型；</li>
<li>在学习过程中，要学会区分知识点的重要程度，例如词嵌入，在未学习之前，只是将它看作一个小小的算法或者表示方法，并未觉得它有多重要，所以在学习的时候也打算快速略过，但到后来才明白它在深度学习中非常非常重要，甚至是必要的基础，所以，在学习知识点的时候，需要培养自己对重要知识点的<b>敏感性</b>和<b>区分能力</b>，例如，在学习这个知识点之前，就了解到词嵌入是NLP的里程碑之一，而且从Andrew Ng在深度学习课程中的目录来看，专门花了一周的时间来讲，但自己都没有引起重视，这都是自己敏感性和区分能力不强的体现；   </li>
</ul>
<p><b id="log">【遗】</b></p>
<ul>
<li style="list-style: none"><input type="checkbox"> 顺序表的分离式结构是用的一个指针数组+元素存储区还是两个指针数组+元素存储区，元素存储区是分开的还是连续的；</li>
<li style="list-style: none"><input type="checkbox"> Glove词向量还并没有明白原理；</li>
</ul>
<p><b id="log">【待】</b></p>
<ul>
<li style="list-style: none"><input type="checkbox"> 简单实现词嵌入，功能实现上虽然比较简单，但是要考虑到复杂的细节问题；<br><br><br><br><hr>



</li>
</ul>
<h3 id="3-19-链表-c-词嵌入-c"><a href="#3-19-链表-c-词嵌入-c" class="headerlink" title="3.19　链表(c) + 词嵌入(c)"></a>3.19　链表(c) + 词嵌入(c)</h3><p><b id="log">【行】</b></p>
<ul>
<li>词嵌入具体编程实现；</li>
<li>链表刷题“复制复杂的链表“；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li><p>上半天刷链表题，在一道“复制复杂链表”的题上纠结很久，<b>慢</b> 是今天主要问题，总结有如下几个方面的原因：</p>
<ul>
<li><b>题目的解读方面：</b><ul>
<li>“指向任意节点”以为是不确定的指向，但如果仔细一想，不确定的话也就没有办法做了；</li>
<li>“复制”一个链表的含义，复制是如果操作的，没弄清楚就开始思考如何设计；</li>
<li>C++语言夹杂题中干扰理解，应该直接搜网上python题目的描述；</li>
<li>“不返回参数中的节点引用”开始不理解，主要是对哪些是参数，引用是什么没有理解透彻；<br>其实《数据结构与算法：Python语言描述》绪论一章中的“交叉路口的红绿灯安排”就是一个很好的程序设计模板，应该仔细反复阅读理解；</li>
</ul>
</li>
<li><b>问题的分析方面：</b><ul>
<li>此题的问题关键在于分析复制原链表之后的random指针复制问题，一般能想到的简单的方法这个环节的复杂度都在$O(n^2)$,所以优化应该优化这个环节，找到这个环节之后就应该考虑用哪个数据结构，这时候对各种数据结构的效率的熟悉程度能够很大程度上为自己提供解决方案备选；</li>
<li>找到方案之后评估算法复杂度是一个重要环节，今天就是因为把复杂度弄错了导致理解了半天；</li>
<li>特殊情况的考虑自己今天在设计程序时没有考虑，又是一拿到题就开始做，《剑指Offer》中程序的设计流程又忘了，这个应该在以后每次考虑问题时都要遵守流程，防止自己漏掉重要环节，导致程序的鲁棒性差；</li>
</ul>
</li>
<li><b>编程方面：</b><ul>
<li>变量的命名不够言简意赅；</li>
<li>调试方法不够熟练（准确来说是基本没有用到专业的方法），这个在前几天就说过了，但依旧没有开始使用，<del>还在使用自己的比较笨的办法(添加print())；</del> （改：这种print()方法不是笨，反而有时候很有必要，所以这只是所适应的情况不同而已。）</li>
<li>在搜到的样本代码中涉及到了map函数，这本应该在python语言学习中学过的，但是到现在也没有复习到那里（廖雪峰python教程），这个得利用空隙时间抓紧复习；</li>
</ul>
</li>
</ul>
</li>
<li><p>在写某个算法或者说模型的时候，遇到还<b>没有学过的</b>，<b>不重要的模块</b>，可以直接找现成的使用，没有必要 <b>非黑即白</b> 非要个人全部实现或者全部照着别人的代码打，可以部分使用已有的<b>“轮子”</b>，<b>把主要精力放在关键原理的实现上</b>；</p>
</li>
<li>今天的代码基本上都是照着别人的代码敲的，自己还没有完全独立的实现；        </li>
</ul>
<p><b id="log">【待】</b></p>
<ul>
<li style="list-style: none"><input type="checkbox"> 复杂链表复制的那道题中样本代码的递归解法没有理解，哈希解法也没有看完；<br><br><br><br><hr>




</li>
</ul>
<h3 id="3-20-链表-c-词嵌入-c"><a href="#3-20-链表-c-词嵌入-c" class="headerlink" title="3.20　链表(c) + 词嵌入(c)"></a>3.20　链表(c) + 词嵌入(c)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer》刷题 以$O(1)$时间删除指定链表节点</li>
<li>弄清word2vec 实现的一些细节；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>刷题的时候思路应该突破常规，学习解题方法是从哪些角度突破常规的；<br><br><br><br><hr>




</li>
</ul>
<h3 id="—-3-21-阶段性小结-—"><a href="#—-3-21-阶段性小结-—" class="headerlink" title="—　3.21　阶段性小结　—"></a>—　3.21　阶段性小结　—</h3><p><center><b>学习路线及所购书籍</b></center><br><img src="/DIY/picture/【日志】03.png" alt="link"><br><img src="/DIY/picture/【日志】04.png" alt="link"><br><b id="em">存在问题及解决对策：</b></p>
<ul>
<li><b>浮躁</b>，这是最大的问题，没有能静下心来沉淀一些东西；</li>
<li><b>暂停学习</b>，每次连续学习的时候，都会出现一段时间的学习中断，主要原因可能是兴趣不高，只把这个学习过程当成一种任务来完成，所以<b>如何提高兴趣是关键</b>，把模型应用于实践是一个提高兴趣的方式；</li>
<li>对于很多课程和书籍上的内容，都只是<b>停留在了理论阶段</b>，理解得本来不一定准确到位，但是这又是一个<b>工程性、实践性</b>比较强的领域，所以导致了很多概念在学习了不久之后就忘了；另一方面，只停留在理论上一定程度地减少了学习的兴趣，实现一个模型应用于实际往往能提高对这个领域的兴趣；</li>
<li>在学习的过程中经常将精力花在<b>“形式”</b>方面的东西，比如制作一个图表，花大把时间去美化这个表等等诸如类似的行为，<b>没有把主要精力和时间放在重点内容上，本末倒置了</b>；</li>
<li>购买学习资料很多时候都不是立马所需的，买的资料很多都没有看完甚至是没有看，<b>没有根据自己现阶段的需求进行购买</b>；</li>
<li>学习资料的选择很重要，尽量避免培训式课程；</li>
<li>花时间打牢基础比追求最新进度更为重要；</li>
</ul>
<p><br><br><br></p>
<hr>





<h3 id="3-22-栈-t-循环序列模型-t"><a href="#3-22-栈-t-循环序列模型-t" class="headerlink" title="3.22　栈(t) + 循环序列模型(t)"></a>3.22　栈(t) + 循环序列模型(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《数据结构与算法：Python结构描述》栈理论一节</li>
<li>Andrew Ng的DeepLearning第五门课第一周内容 - 循环序列模型</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>学习高级数据结构的底层实现或者说其他知识点的底层实现，更有利于理解和运用高层次的知识技能，比如在学习栈一节的时候，递归与非递归之间的转换底层其实就是通过栈来存储中间数据来实现，所以，学习知识点要<b>抓住本质</b>；</li>
<li>今天在学习吴恩达的深度学习循环神经网络一节的时候，明显没有以前那么困难了，归结原因主要有两点：<ul>
<li>主动性思考，主动思考原理中的参数是代表什么，对于模糊的地方进行推理，记住存在的可能性有几种，加深对原理的理解；</li>
<li>基础知识掌握比较牢固，以前学习这个的时候，对于标准的神经网络中的公式不能够完全理解，在重新复习掌握一遍之后，今天对于RNN中很多东西就能够自己快速理解了；</li>
</ul>
</li>
</ul>
<p><b id="log">【待】</b></p>
<ul>
<li style="list-style: none"><input type="checkbox" checked> 栈的编程实现</li>
<li style="list-style: none"><input type="checkbox"> 循环序列模型的理论完善和实现<br><br><br><br><hr>





</li>
</ul>
<h3 id="3-23-栈-c-RNN和LSTM-t"><a href="#3-23-栈-c-RNN和LSTM-t" class="headerlink" title="3.23　栈(c) + RNN和LSTM(t)"></a>3.23　栈(c) + RNN和LSTM(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>RNN和LSTM理论深入细节</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>学习《数据结构与算法：Python语言描述》中的结构、算法、例题时，学习思考结构设计的分析过程也非常重要，这里面的例题都比较经典；</li>
</ul>
<p><b id="log">【遗】</b></p>
<ul>
<li style="list-style: none"><input type="checkbox" checked> RNN和LSTM及GRU的门控制是为了什么还是没有弄得太清楚；</li>
<li style="list-style: none"><input type="checkbox" checked> 栈的应用中实现的后缀表达式例题代码还是有点问题；</li>
</ul>
<p><b id="log">【待】</b></p>
<ul>
<li style="list-style: none"><input type="checkbox"> 错误处理类的学习；</li>
<li style="list-style: none"><input type="checkbox"> print()函数输出变量写法；</li>
<li style="list-style: none"><input type="checkbox"> 写一个像前段时jieba简单版的程序框架，熟悉整体布局和设计；</li>
<li style="list-style: none"><input type="checkbox"> 练习调试节点的设置，增强debug能力；<br><br><br><br><hr>




</li>
</ul>
<h3 id="3-24-队列-t-seq2seq和注意力机制-t"><a href="#3-24-队列-t-seq2seq和注意力机制-t" class="headerlink" title="3.24　队列(t) + seq2seq和注意力机制(t)"></a>3.24　队列(t) + seq2seq和注意力机制(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>学习《数据结构与算法：Python语言描述》中的队列及其相关概念</li>
<li>Andrew Ng的DeepLearning第五门课第三周内容 - 序列模型和注意力机制</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>数据结构和算法的很多思想可以迁移到某些模型的计算设计中，比如递归算法的思想可以用到动态规划的维特比算法最初的设计启发中；</li>
<li>学习深度学习相关章节时，有点浮躁，急着想看完视频；</li>
</ul>
<p><b id="log">【待】</b></p>
<ul>
<li>进一步学习RNN、LSTM、GRU、seq2seq、注意力机制的细节问题；</li>
<li>完成队列相关内容的编程，完善栈未debug完的后缀表达式例题；<br><br><br><br><hr>




</li>
</ul>
<h3 id="3-25-栈-c-队列-c-注意力模型-t"><a href="#3-25-栈-c-队列-c-注意力模型-t" class="headerlink" title="3.25　栈(c) + 队列(c) + 注意力模型(t)"></a>3.25　栈(c) + 队列(c) + 注意力模型(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>Attention机制深入理解</li>
<li>门函数理解</li>
<li>复习吴恩达DeepLearning第五门课第三周内容</li>
<li>“实现”栈的debug、背包问题、简单队列、迷宫</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>养成 <b>独立地分析问题，完成整个代码</b> 的习惯，<b>不要总照着样本代码敲</b>，可以先看资料上问题的分析流程，然后尝试着自己写代码，如果写不出来，再参考<b>部分样本代码的实现</b>，然后接着自己再写，这样反复循环这个过程，直到写出整个功能实现，再回过头来对比自己的代码和样本代码，分析自己和样本代码的区别；<br><br><br><br><hr>





</li>
</ul>
<h3 id="—-3-26-短期计划-—"><a href="#—-3-26-短期计划-—" class="headerlink" title="—　3.26　短期计划　—"></a>—　3.26　短期计划　—</h3><p><b id="em">应聘模块：</b></p>
<ul>
<li>搜集学习成篇的面试内容，抓住常考重点；</li>
<li>查看招聘信息总结重点；</li>
</ul>
<p><b id="em">编程模块：</b><br>把更多的时间分配到刷题上面，适当减少纯阅读学习的时间，必须保证每天刷题，以下为重点：</p>
<ul>
<li>训练解题思路（注意高压下的训练）；</li>
<li>熟悉Python语言；</li>
<li>熟悉数据结构与算法；</li>
</ul>
<p><b id="em">NLP模块：</b></p>
<ul>
<li>常用算法：HMM、最大熵模型、CRF、EM、PCFG、LDA</li>
<li>基本环节：文本预处理、分词、词性标注、命名实体识别、句法依存分析、句法浅层分析、语义角色标注</li>
<li>常见任务：文本分类、主题建模、信息抽取、实体识别、情感分析</li>
<li>应用层面：问答、对话</li>
<li>开源工具NLTK等的使用</li>
</ul>
<p><b id="em">深度学习模块：</b></p>
<ul>
<li>Tensorflow框架的学习</li>
<li>深度学习基础的学习（Andrew Ng深度学习的第2门课 4.8h）</li>
<li>机器学习策略的学习（Andrew Ng深度学习的第3门课 3.4h）</li>
<li>深度学习简单应用系统的搭建（参考CS224n课程）</li>
<li>TextCNN ？</li>
</ul>
<p><b id="em">机器学习模块：</b></p>
<ul>
<li>scikit-learn的使用;<br></li>
<li>常用算法：SVM、决策树、朴素贝叶斯、KNN、Kmeans;<br></li>
<li>其他概念：过拟合、偏差与方差、正则化（L1和L2）、查准率和查全率、损失函数<br><br><br><br><hr>





</li>
</ul>
<h3 id="3-26-栈和队列-c-深度学习-t-Python-t"><a href="#3-26-栈和队列-c-深度学习-t-Python-t" class="headerlink" title="3.26　栈和队列(c) + 深度学习(t) + Python(t)"></a>3.26　栈和队列(c) + 深度学习(t) + Python(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>栈和队列的刷题</li>
<li>Andrew Ng的DeepLearning第二门课第一周的内容；</li>
<li>Python装饰器</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>记录刷题的时候每道题所用的时间；</li>
<li>在写代码的过程中，经常忘了给函数加“（）”调用符号，导致一些不好查找的非语法错误；</li>
<li>刷题具体分析题目时，一定要一步步分析，而不是像下面自己操作的那样“跳着走”；<br><img src="/DIY/picture/【日志】07.png" alt="link"></li>
</ul>
<p><b id="em">遗问：</b></p>
<ul>
<li style="list-style: none"><input type="checkbox" checked> 梯度和步长<br><br><br><br><hr>





</li>
</ul>
<h3 id="3-27-刷题-c-深度学习-t-机器学习-t"><a href="#3-27-刷题-c-深度学习-t-机器学习-t" class="headerlink" title="3.27　刷题(c) + 深度学习(t) + 机器学习(t)"></a>3.27　刷题(c) + 深度学习(t) + 机器学习(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer》刷一题</li>
<li>Andrew Ng的DeepLearning 第二门课第二周</li>
<li>EM公式推导</li>
<li>重新理解梯度、梯度为何能求损失函数</li>
<li>python语言</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>深入理解Python语言各数据结构的特性和功能才能更好更快地编码；</li>
<li>多用<a href="https://www.geogebra.org/" target="_blank" rel="noopener">Geogebra</a>，建立空间概念；</li>
<li>学习某种算法时，弄清楚基本数学组件后，理解模型才能得心应手，理解清楚每一步在做什么，每个数学符号的含义，这样才能更顺畅地理解，例如今天重新学习推理EM（最大期望值）算法的过程，再有今天推导公式时重要的经验之一就是符号的书写、排版可能影响到了思考思路；</li>
<li>分段记录刷题时的分析时间，编码时间；</li>
<li>刷题时理解透彻，思路清晰之后再编码，不要急着编码思路都不清楚；</li>
<li>学习某种算法或者模型前，最好先把其中涉及到的预备知识点都学习好，避免学习模型途中产生理解断点；<br><br><br><br><hr>




</li>
</ul>
<h3 id="3-28-Python-t-二叉树-t-EM算法-t-刷题-c-深度学习-t"><a href="#3-28-Python-t-二叉树-t-EM算法-t-刷题-c-深度学习-t" class="headerlink" title="3.28　Python(t) + 二叉树(t) + EM算法(t) + 刷题(c) + 深度学习(t)"></a>3.28　Python(t) + 二叉树(t) + EM算法(t) + 刷题(c) + 深度学习(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>Python语言</li>
<li>《数据结构与算法：Python描述》二叉树相关章节</li>
<li>EM算法理解和公式推导及实例推导</li>
<li>《剑指Offer》刷题</li>
<li>Andrew Ng的DeepLearning第二门课第三周内容</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>分析题目时先举例子试着“走几步”进行分析，然后再写白板编程或者中文描述代码流程，最后再编代码；</li>
<li>先写测试用例，再写程序，保证程序的健壮性、鲁棒性；</li>
<li>刷题时注意算法复杂度的分析，不要只顾着相处解决方案就了事；</li>
<li>系统复习Python重要数据结构的属性和功能函数，在最近的编码中都发现了对基本的list、tuple、字符串、set、dict功能函数和属性都不熟悉；</li>
</ul>
<p><b id="log">【遗】</b></p>
<ul>
<li style="list-style: none"><input type="checkbox"> batch-normalization了解得不是很透彻，需要以后再理解；<br><br><br><br><hr>




</li>
</ul>
<h3 id="3-29-EM算法-t-刷题-c"><a href="#3-29-EM算法-t-刷题-c" class="headerlink" title="3.29　EM算法(t) + 刷题(c)"></a>3.29　EM算法(t) + 刷题(c)</h3><p><b id="log">【行】</b></p>
<ul>
<li>Python语言</li>
<li>EM算法公式的深入理解</li>
<li>《剑指Offer》刷题</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>积累一些好的网络资源，比如看到一些好的博文，可以顺带看看博文的参考资料，看看博主的其他文章和博主关注的一些好的博文或者其他博主；</li>
<li>注意复习刚学过的东西，避免又忘了，<b>思考遗忘的根本原因</b>；</li>
<li>找一些提高记忆力的方法；</li>
<li>保持精力充沛-注意睡眠、运动、饮食的调整，<b>学习时间、精力的管理</b>，这点可以看看德鲁克的管理类书籍；</li>
<li>状态有些松懈；<br><br><br><br><hr>




</li>
</ul>
<h3 id="3-30-HMM-t"><a href="#3-30-HMM-t" class="headerlink" title="3.30　HMM(t)"></a>3.30　HMM(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>Python语言</li>
<li>HMM模型深入全面性理解</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>状态有些松懈；</li>
<li><b>学习不是按部就班的工厂流水线，有些细节需要一气呵成。</b>这句话更能体现专注和兴趣；</li>
</ul>
<p><b id="log">【待】</b></p>
<ul>
<li>思考为什么比原来理解数学更深更快了；<br><br><br><br><hr>





</li>
</ul>
<h3 id="3-31-深度学习-t"><a href="#3-31-深度学习-t" class="headerlink" title="3.31　深度学习(t)"></a>3.31　深度学习(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>Andrew Ng的深度学习第三门课第一周 1.4-1.7节</li>
</ul>
<p><b id="log">【待】</b></p>
<ul>
<li>思考是否更针对面试来学习知识；<br><br><hr>






</li>
</ul>
<h2 id="2019年4月"><a href="#2019年4月" class="headerlink" title="2019年4月"></a>2019年4月</h2><h3 id="—-4-01-短期计划-—"><a href="#—-4-01-短期计划-—" class="headerlink" title="—　4.01　短期计划　—"></a>—　4.01　短期计划　—</h3><p><b id="em">应聘模块：</b><br>加分项：</p>
<ul>
<li>ML策略</li>
<li>Linux工作环境</li>
</ul>
<p><b id="em">Coding模块：</b></p>
<ul>
<li>《剑指Offer》</li>
<li>《数据结构与算法：Python语言描述》散列表、图、排序、查找、平衡二叉树</li>
</ul>
<p><b id="em">ML模块：</b></p>
<ul>
<li>SVM、朴素贝叶斯、决策树、K-means、KNN、提升算法、感知机、逻辑回归、SVD</li>
<li>ML库Scikit-learn –&gt; 莫烦</li>
</ul>
<p><b id="em">NLP模块：</b></p>
<ul>
<li>HMM、EM、MaxEnt、CRF、LDA、PCFG</li>
<li>分词、标注、实体识别、浅层句法</li>
<li>应用：文本分类（《Python文本分析》)、流程（预处理+ … ）</li>
</ul>
<p><b id="em">DL模块：</b></p>
<ul>
<li>Word-Embedding、RNN、LSTM、GRU、Attention、Transformer、自编码器（非监督学习）、递归神经网络</li>
<li>框架选择：<ul>
<li style="list-style: none"><input type="checkbox" checked> Pytorch –&gt; 莫烦<br><del>Tensorflow</del></li>
</ul>
</li>
<li>CNN –&gt; Andrew Ng 第三门课前两周视频<br><br><br><br><hr>





</li>
</ul>
<h3 id="4-01-最大熵模型-t-方向导数和梯度-t"><a href="#4-01-最大熵模型-t-方向导数和梯度-t" class="headerlink" title="4.01　最大熵模型(t) + 方向导数和梯度(t)"></a>4.01　最大熵模型(t) + 方向导数和梯度(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《自然语言处理综论》最大熵模型一节</li>
<li>方向导数和梯度（网络PPT）</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>做事不果断，没有分清主次；<br><br><br><br><hr>





</li>
</ul>
<h3 id="4-02-拉格朗日乘子法与对偶性-t"><a href="#4-02-拉格朗日乘子法与对偶性-t" class="headerlink" title="4.02　拉格朗日乘子法与对偶性(t)"></a>4.02　拉格朗日乘子法与对偶性(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>拉格朗日乘子法与对偶性(博文学习)</li>
<li>修改4.01短期计划<br><br><br><br><hr>





</li>
</ul>
<h3 id="4-03-深度学习-t-PyTorch-c"><a href="#4-03-深度学习-t-PyTorch-c" class="headerlink" title="4.03　深度学习(t) + PyTorch(c)"></a>4.03　深度学习(t) + PyTorch(c)</h3><p><b id="log">【行】</b></p>
<ul>
<li>Andrew Ng 深度学习 第三门课第一周内容</li>
<li>PyTorch莫烦系列（前17个短视频）<br><br><br><br><hr>





</li>
</ul>
<h3 id="4-04-修改短期计划-深度学习-t-PyTorch-c-《自然语言处理综论》-t"><a href="#4-04-修改短期计划-深度学习-t-PyTorch-c-《自然语言处理综论》-t" class="headerlink" title="4.04　修改短期计划 + 深度学习(t) + PyTorch(c) + 《自然语言处理综论》(t)"></a>4.04　修改短期计划 + 深度学习(t) + PyTorch(c) + 《自然语言处理综论》(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>Andrew Ng 深度学习 第四门课第一周内容</li>
<li>Pytorch莫烦系列编码</li>
<li>《自然语言处理综论》 第22章 信息抽取</li>
</ul>
<p><b id="em">修改短期计划优先度：</b><br>(1) 数据结构算法和刷题是必要先做的，笔试必考，TCP/IP三次握手掌握；<br>(2) 项目经验：文本分析应用;<br>(3) NLP算法、机器学习算法；（上网搜考点）<br>(4) PyTorch、深度学习编码；<br>(5) 粗览《自然语言处理综论》中的摘要和对话章节；<br><br><br><br></p>
<hr>






<h3 id="4-05-字典-t-刷题-c-CRF-t"><a href="#4-05-字典-t-刷题-c-CRF-t" class="headerlink" title="4.05　字典(t) + 刷题(c) + CRF(t)"></a>4.05　字典(t) + 刷题(c) + CRF(t)</h3><p><b id="log">【行】</b>：</p>
<ul>
<li>《数据结构与算法：Python语言描述》字典一节</li>
<li>《剑指Offer》面试题：矩阵中的路径</li>
<li>CRF算法学习<br><br><br><br><hr>





</li>
</ul>
<h3 id="4-06-散列表-t-刷题-c-CRF-MaxEnt复习-t"><a href="#4-06-散列表-t-刷题-c-CRF-MaxEnt复习-t" class="headerlink" title="4.06　散列表(t) + 刷题(c) + CRF/MaxEnt复习(t)"></a>4.06　散列表(t) + 刷题(c) + CRF/MaxEnt复习(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《数据结构与算法：Python语言描述》散列表一节</li>
<li>《剑指Offer》面试题：机器人的运动范围</li>
<li>弄懂CRF/MaxEnt模型</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>日本人写的书籍大多比较精细，可以多看看他们的书籍和资料等等；</li>
</ul>
<p><b id="log">【待】</b></p>
<ul>
<li>IIS迭代算法；</li>
<li>HMM训练问题；</li>
<li>在谷歌浏览器一个收藏的书签中学习判别式模型和生成式模型；</li>
<li>补充学习指数族分布；<br><br><br><br><hr>





</li>
</ul>
<h3 id="4-07-集合-t-字符串-t-朴素贝叶斯-t"><a href="#4-07-集合-t-字符串-t-朴素贝叶斯-t" class="headerlink" title="4.07　集合(t) + 字符串(t) + 朴素贝叶斯(t)"></a>4.07　集合(t) + 字符串(t) + 朴素贝叶斯(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《数据结构与算法：Python语言描述》集合一节</li>
<li>《数据结构与算法：Python语言描述》字符串一节（未完）</li>
<li>《统计学习方法》朴素贝叶斯复习</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>在学《数据结构与算法：Python语言描述》一书中，总觉得懂得<b>模式设计</b>非常重要；</li>
<li>要记录在学习过程中因为某些原因跳过的内容，并且记录为什么跳过；</li>
<li>对于学过的知识点应该<b>温故而”知新”</b>，这个新是指更深层次或者更多角度的理解；</li>
<li>精读好的书籍，了解它们的思路和细节，并且知道它们各自的优缺点：<ul>
<li>《统计学习方法》 –&gt; 精细而不够全面</li>
<li>《机器学习》 –&gt; 全面而不够精细</li>
<li>《自然语言处理综论》 –&gt; 全面而精细，但是有点“过时”</li>
</ul>
</li>
<li>每本讲模型和算法的书都基本会在目录的附近写有自己公式中各个字母的含义，而这些解释恰恰是理解公式的前提，以前自己都忽略了，以至于在理解公式的时候总是猜测几种可能然后验证，但这样浪费太多时间而且有可能理解出现偏差；</li>
<li><b>以慢为快</b> 理解、吃透前面的知识点和基础，再往后面进行，不要一味图快；<br><br><br><br><hr>





</li>
</ul>
<h3 id="4-08-字符串-t-刷题-c-SVM-t"><a href="#4-08-字符串-t-刷题-c-SVM-t" class="headerlink" title="4.08　字符串(t) + 刷题(c) + SVM(t)"></a>4.08　字符串(t) + 刷题(c) + SVM(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《数据结构与算法：Python语言描述》字符串一节</li>
<li>《剑指Offer》二进制中1的个数、剪绳子</li>
<li>结合《统计学习方法》、Andrew Ng机器学习、黄志洪机器学习、《数据挖掘导论》综合学习SVM，但有些问题依旧没有弄得很清楚；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>结合不同的书籍理解同一个概念，每本书的侧重点都有所不同，像今天理解《统计学习方法》中SVM一节，纯数学公式太多，并且李航本身用的符号表示比较复杂，所以导致不能够很好理解，然后再看Andrew Ng的机器学习课程，讲得比较通俗，但是很多细节方面的东西没有讲到，接着又看了黄志洪的机器学习，他讲得有些地方也比较抽象，最后看了他介绍的《数据挖掘导论》这本书上的SVM讲解，总算理解得比较全面和深入了，所以说，理解某些知识点，最好结合多个方向的资料，最好是比较权威的那种来帮助理解，这样能各取所长，综合学习；</li>
<li>多参加一些比赛，了解行业状况，不管能不能获奖，吸取经验是关键；</li>
<li><p id="underline3">最近一两年，应该把学习重点放在数学方向；</p><br><br><br><br><hr>





</li>
</ul>
<h3 id="4-09-刷题-c-Pytorch-t-依存句法分析-t"><a href="#4-09-刷题-c-Pytorch-t-依存句法分析-t" class="headerlink" title="4.09　刷题(c) + Pytorch(t) + 依存句法分析(t)"></a>4.09　刷题(c) + Pytorch(t) + 依存句法分析(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>Leetcode刷题</li>
<li>Pytorch使用</li>
<li>依存句法分析</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>正则表达式是一种语言表达；<br><br><br><br><hr>






</li>
</ul>
<h3 id="4-10-刷题-c-Pytorch-c"><a href="#4-10-刷题-c-Pytorch-c" class="headerlink" title="4.10　刷题(c) + Pytorch(c)"></a>4.10　刷题(c) + Pytorch(c)</h3><p><b id="log">【行】</b></p>
<ul>
<li>刷Leetcode</li>
<li>Pytorch使用手册查看</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>规范代码格式，适当书写注释；</li>
<li>熟记错误代码；</li>
<li>提醒查看《具体数学》经典书籍；<br><br><br><br><hr>




</li>
</ul>
<h3 id="4-11-应聘计划"><a href="#4-11-应聘计划" class="headerlink" title="4.11　应聘计划"></a>4.11　应聘计划</h3><p><b id="em">编码环节：</b></p>
<ul>
<li>《剑指Offer》结合LeetCode刷题：在LeetCode官网上按照《剑指Offer》的模块进行有针对性、目的性地刷题，并形成解题思路和总结，不要在一道题上纠缠过多的时间，要多刷，刷几遍；</li>
<li><p>数据结构与算法重要顺序：</p>
<ul>
<li>第一梯队：复杂度分析、链表、二叉树、二分查找、快速排序、归并排序、回溯法、动态规划、分治</li>
<li>第二梯队：栈、队列、堆、图 <font size="1px">{DFS遍历、BFS遍历、最大（小）生成树算法（Kruskal、Prim）、最短路径（Dijkstra、Floyd）}</font></li>
<li><p>第三梯队：哈希、集合</p>
<p>这个阶段的复习不用看《数据结构与算法：Python语言描述》了，不懂的就上网查一下，关于Python语言的研究也可以暂时停止，因为在这个阶段主要是学习算法，python中的一些高级结构封装了算法，不适合在这里使用；</p>
</li>
</ul>
</li>
</ul>
<p><b id="em">项目环节：</b></p>
<ul>
<li>选择<b> 信息抽取 </b> 作为学习项目；因为信息抽取一方面集合了许多底层的基础知识比如分词、词性标注、实体识别、消歧等等，另一方面，它又作为了许多更高级的应用的基础，现在研究这些高级应用能力还不够，所以先从信息抽取开始，并且在招聘中，信息抽取是许多公司要求的；</li>
<li>文本分类（情感分类）作为加分项，因为招聘中也有许多公司对这方面有需求，这方面的知识知道具体流程和方法就行了，没有必要编程；</li>
</ul>
<p><b id="em">NLP知识环节</b></p>
<ul>
<li>HMM、EM、MaxEnt、CRF、LDA：重点记忆理解其中的数学公式；</li>
<li>分词、标注、实体识别、依存句法分析：了解主要的方法有哪些，做成结构图进行记忆；</li>
<li>熟悉一种自然语言处理软件的使用；</li>
</ul>
<p><b id="em">DL知识环节：</b></p>
<ul>
<li>Word-Embedding（最为重要）及genism的使用、RNN、LSTM、GRU、Attention、Transformer、自编码器（非监督学习）、递归神经网络：重点记住数学公式；</li>
<li>框架 Pytorch：利用莫烦的视频进行简单学习，结合Pytorch官网进行学习；</li>
<li>CNN –&gt; Andrew Ng 第四门课第二周视频</li>
<li>CS224n的实现代码作为NLP在Pytorch中的实现和练习；</li>
</ul>
<p><b id="em">ML知识环节：</b></p>
<ul>
<li>找ML面试题进行学习；</li>
<li>SVM、朴素贝叶斯、决策树、K-means、KNN、提升算法、感知机、逻辑回归、SVD</li>
<li>ML库Scikit-learn –&gt; 莫烦</li>
</ul>
<p><b id="em">加分环节和其他环节</b></p>
<ul>
<li>计算机网络知识</li>
<li>ML策略</li>
<li>Linux工作环境<br><br><br><br><hr>





</li>
</ul>
<h3 id="4-12-刷题-c-信息抽取-t"><a href="#4-12-刷题-c-信息抽取-t" class="headerlink" title="4.12　刷题(c) + 信息抽取(t)"></a>4.12　刷题(c) + 信息抽取(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer》结合牛客网刷题</li>
<li>《自然语言处理综论》信息抽取一章</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>总结刷题的各种技巧和思路启发式怎样的；<br><br><br><br><hr>





</li>
</ul>
<h3 id="4-13-刷题-c-信息抽取及其NLP他应用了解-t"><a href="#4-13-刷题-c-信息抽取及其NLP他应用了解-t" class="headerlink" title="4.13　刷题(c) + 信息抽取及其NLP他应用了解(t)"></a>4.13　刷题(c) + 信息抽取及其NLP他应用了解(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer》结合牛客网刷题</li>
<li>上网查各种信息抽取、文本分析、文本挖掘等等NLP的领域应用，考虑发展方向</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>认真思考为什么想要做这行，到底追求些什么东西，然后再来行动；</li>
</ul>
<p><br><br><br></p>
<hr>

<h3 id="4-15-刷题-c-Pytorch学习-t"><a href="#4-15-刷题-c-Pytorch学习-t" class="headerlink" title="4.15　刷题(c) + Pytorch学习(t)"></a>4.15　刷题(c) + Pytorch学习(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer》刷题，递归类题目</li>
<li>Pytorch熟悉流程和函数<br><br><br><br><hr>






</li>
</ul>
<h3 id="4-16-应聘计划-修改"><a href="#4-16-应聘计划-修改" class="headerlink" title="4.16　应聘计划(修改)"></a>4.16　应聘计划(修改)</h3><p><b>戒躁，踏踏实实学好每一个知识点</b><br><b id="em">编码环节：</b></p>
<ul>
<li>《剑指Offer》结合LeetCode刷题：在LeetCode官网上按照《剑指Offer》的模块进行有针对性、目的性地刷题，并形成解题思路和总结，不要在一道题上纠缠过多的时间，要多刷，刷几遍；</li>
<li><p>数据结构与算法重要顺序：</p>
<ul>
<li>第一梯队：复杂度分析、链表、二叉树、二分查找、快速排序、归并排序、回溯法、动态规划、分治、正则表达式使用</li>
<li>第二梯队：栈、队列、堆、图 <font size="1px">{DFS遍历、BFS遍历、最大（小）生成树算法（Kruskal、Prim）、最短路径（Dijkstra、Floyd）}</font></li>
<li><p>第三梯队：哈希、集合</p>
<p>这个阶段的复习不用看《数据结构与算法：Python语言描述》了，不懂的就上网查一下，关于Python语言的研究也可以暂时停止，因为在这个阶段主要是学习算法，python中的一些高级结构封装了算法，不适合在这里使用，这里学习的应该是所有语言通用的实现方式；</p>
</li>
</ul>
</li>
</ul>
<p><b id="em">项目环节：</b></p>
<ul>
<li>文本分类（情感分类）；</li>
<li>分词、标注、实体识别、依存句法分析；</li>
</ul>
<p><b id="em">NLP知识环节</b></p>
<ul>
<li>HMM、EM、MaxEnt、CRF、LDA：重点记忆理解其中的数学公式；</li>
<li>熟悉一种自然语言处理软件的使用（NLTK）和一种中文NLP软件LTP的使用；</li>
</ul>
<p><b id="em">DL知识环节：</b></p>
<ul>
<li>Word-Embedding（最为重要）及genism的使用、RNN、LSTM、GRU、Attention、Transformer、自编码器（非监督学习）、递归神经网络：重点记住数学公式；</li>
<li>框架 Pytorch：利用莫烦的视频进行简单学习，结合Pytorch官网进行学习；</li>
<li>CNN –&gt; Andrew Ng 第四门课第二周视频</li>
<li>CS224n的实现代码作为NLP在Pytorch中的实现和练习；</li>
</ul>
<p><b id="em">ML知识环节：</b></p>
<ul>
<li>找ML面试题进行学习；</li>
<li>SVM、朴素贝叶斯、决策树、K-means、KNN、提升算法、感知机、逻辑回归、SVD、PCA</li>
<li>ML库Scikit-learn –&gt; 莫烦</li>
</ul>
<p><b id="em">加分环节和其他环节</b></p>
<ul>
<li>计算机网络知识</li>
<li>ML策略</li>
<li>Linux工作环境</li>
<li>Python语言自身<br><br><br><br><hr>




</li>
</ul>
<h3 id="4-22-写给快28岁的自己"><a href="#4-22-写给快28岁的自己" class="headerlink" title="4.22　写给快28岁的自己"></a>4.22　写给快28岁的自己</h3><p>　　不知不觉，来到这个世界已经近28个年头，好像自己什么都没做，什么也没有做成，本该是工作了几年的自己，如今却仍未工作，究其缘由只是因为还没有找到属于自己的路。近段段时间的AI兴起，让自己自己好像突然有了方向，但奈何多重原因，至今也未有个实质性的进展，同时也浪费了大量时间。最近结了婚，有了妻子，似乎下定决定要改变这一现状，但现实是心态浮躁，仍未工作。自我感觉不能够再这样下去了，为了老婆，也为了自己。现对以前的自己做如下总结和规划，希望能够从中吸取教训，<b>收拾行囊，重新出发！</b></p>
<p><b>Q：为什么选择NLP作为以后的工作方向？</b></p>
<ul>
<li><p id="underline3">NLP涉及了哲学、认知、心理等物种（不仅仅是人类）层面最根本性的东西，它的推进，将有利于推进人类发展的进程，这是一件很酷的事情；</p></li>
<li>选择AI中的的NLP是因为NLP在AI中属于认知层面的，相对于与感知层面的技术比如语音和视觉，它仍处于萌芽的状态，很多技术都不成熟，这就为自己赶上前沿技术走在前列争取了时间；</li>
<li>人工智能正在深刻地改变人们的工作、生活方式，这是一个将来非常重要的领域，每个人都应该在一定程度上了解甚至掌握这门技术;</li>
<li>从事这个领域的薪资待遇也非常不错；</li>
</ul>
<p><b>Q：为什么学了这么长时间的NLP却还是没能够掌握？</b></p>
<ul>
<li>只重视理论，不重视实践。在学习的时候，可能理解了知识点，但基本上很少用代码去实现，这样就造成了以下几个结果：<blockquote>
<p>(a) 理论知识可能没有正确理解，本来在编程环节可以发现的（也就是说编程可以促进理解），但是没有编程，导致了可能理解错误；<br>(b) 不懂得如何去实现理论，即使是造好的轮子，也不知道该使用哪些软件工具；<br>(c) 遗忘知识点，理解不深刻，没有编程，自然而言导致记忆不深刻；</p>
</blockquote>
</li>
<li>不重视基础，基础理论没有吃透（例如EM算法在网络博文中有一篇提到了九层理解境界，但自己却止步于一二层），而高级模型正是有这些“积木”搭建起来的，基础不牢，导致的结果有以下几个：<blockquote>
<p>(a) 理解慢：要花很长时间去理解高级模型；<br>(b) 延展性差：针对不同的应用场景，很难去修改高级模型细节去适应新场景；</p>
</blockquote>
</li>
</ul>
<p><b>Q：学习或者生活中影响做事的习惯或者思想有哪些？</b></p>
<ul>
<li>做事情拖沓；</li>
<li>时间和精力的分配管理做得很差，经常是做一件事的时候会分神去做与这件事可能不相关或者相关但不重要事情上去，本末倒置把主要的精力和时间浪费了；</li>
<li>学习停滞，缺乏持续性学习的兴趣；</li>
<li>总结了很多学习的方式方法，但从来不去应用；</li>
<li>工作与生活分界不清，抽取独立的时间陪伴家人；</li>
<li>每个阶段的目标性不强，没有进行更为细致的规划，导致在一些问题上精力不集中，造成了时间的浪费；</li>
<li>没有积累的习惯，写代码的时候没有分类存储的长期习惯，总是在一段时间后就把前面的代码删除；</li>
</ul>
<p><b>Q：如何从NLP领域脱颖而出？</b></p>
<ul>
<li>兴趣（长期性原动力）</li>
<li>基础</li>
<li>视野</li>
<li>英语能力<br><br><br><br><hr>





</li>
</ul>
<h3 id="4-23-刷题-c-词嵌入复习和gensim使用-c"><a href="#4-23-刷题-c-词嵌入复习和gensim使用-c" class="headerlink" title="4.23　刷题(c) + 词嵌入复习和gensim使用(c)"></a>4.23　刷题(c) + 词嵌入复习和gensim使用(c)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer》刷题</li>
<li>复习词嵌入</li>
<li>练习使用gensim的word2vec<br><br><br><br><hr>





</li>
</ul>
<h3 id="4-24-刷题-c-词嵌入复习和gensim使用-c"><a href="#4-24-刷题-c-词嵌入复习和gensim使用-c" class="headerlink" title="4.24　刷题(c) + 词嵌入复习和gensim使用(c)"></a>4.24　刷题(c) + 词嵌入复习和gensim使用(c)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer》刷题</li>
<li>归并排序编程</li>
<li>复习词嵌入</li>
<li>练习使用gensim的word2vec<br><br><br><br><hr>





</li>
</ul>
<h2 id="2019年5月"><a href="#2019年5月" class="headerlink" title="2019年5月"></a>2019年5月</h2><h3 id="5-01-刷题-c"><a href="#5-01-刷题-c" class="headerlink" title="5.01　刷题(c)"></a>5.01　刷题(c)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer》刷题</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>在足迹和遗留问题列表中同时记录问题；</li>
<li>各种排序算法的复杂度；</li>
<li>递归结构复杂度分析；<br><br><br><br><hr>





</li>
</ul>
<h3 id="—-5-03-找工作计划-—"><a href="#—-5-03-找工作计划-—" class="headerlink" title="—　5.03　找工作计划　—"></a>—　5.03　找工作计划　—</h3><p></p><p id="underline3">以基础项目促进复习和学习，顺便积累项目经验；</p><br><b id="em">笔试类：</b><p></p>
<ul>
<li>数据结构与算法</li>
<li>Linux系统使用</li>
<li>Python语言本身</li>
<li>少量的计算机网络知识</li>
</ul>
<p><b id="em">面试类：</b></p>
<ul>
<li><p>项目：<br>  （1）在实践中，必须涉及到NLP主流算法模型的使用，包括传统方法和深度学习方法；<br>  （2）系统的编码，要按照正规系统的布局方式来设计（可参考jieba分词系统）<br>  （3）多看看Andrew Ng在deeplearning中的机器学习策略，这是获取经验的捷径，也是能表现得像个很多经验者的方法；</p>
<ul>
<li><p>序列标注：分词系统、词性标注、命名实体识别系统<br>  ==&gt; (bi)LSTM+CRF序列标记项目（设计gensim使用产生词向量，LSTM可以更换为RNN和GRU，CRF训练需要用到HMM中的训练方法，CRF原理与最大熵模型极其相似，使用Pytorch深度学习框架）（参考【ipynb】Deep Learning for Natural Language Processing with Pytorch）</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649447728&amp;idx=1&amp;sn=af6b37d4f7ef9826e768b7fa999e33b1&amp;chksm=82c0b4b4b5b73da2656501ec5ce9d36d1ab213425ca18451e4cf3f8a95aa1f6b387265a0248a&amp;scene=0&amp;xtrack=1&amp;key=ae82afc765e556e1f31e0f8b17c324af38deef2357ee690857c93b62dd5faf8b4582b8984d81ac732af9c517598052612abe668dc36604231e5b4d3cb72ec0278b580ad391c13b654c0879411a009c71&amp;ascene=14&amp;uin=MTkzNzg2MTUxNA%3D%3D&amp;devicetype=Windows+10&amp;version=6206081a&amp;lang=zh_CN&amp;pass_ticket=6JRPZMz8qFg7mHiKmKV6JAd63j5AUTZnVz2h9e1nHlwMICQQZiGqnOC2%2BPxErDiI" target="_blank" rel="noopener">最新中文分词改进版网址</a></li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247497183&amp;idx=1&amp;sn=49ce9df2cf3a33796b64b6dbe268b714&amp;chksm=96ea2a5fa19da34969c85c9100fdcaa8b5e3634b97042266e526e36cf602cf6240faf560c780&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;key=ae6bf02451889696cca8174000da7b33a4061faeb91656bbc5eacdbc62980ee296f7421547f41e565de7c52f2f2f6254e171336d21f7876659590e736614a65a8741416edc11bf7ce80f29ba49a84b2f&amp;ascene=1&amp;uin=MTkzNzg2MTUxNA%3D%3D&amp;devicetype=Windows+10&amp;version=6206081a&amp;lang=zh_CN&amp;pass_ticket=LdxMy%2FDqJPB0yCP7P2nfJ0DCRCjDFln2Gnjv7zNZXBXNN8nMy50qB26WPDRiBioc" target="_blank" rel="noopener">复旦大学邱锡鹏教授：词法、句法分析研究进展综述</a></p>
<p>==&gt; MaxEnt字标注（复习+NLTK的使用）</p>
</li>
</ul>
</li>
<li>句法分析系统（主要是依存句法分析）<br>  ==&gt; 基于图的依存句法分析<br>  ==&gt; 基于转移的依存句法分析</li>
<li>语义分析系统<br>  ==&gt; 指代消解问题</li>
<li>文本聚类与分类（以《Python文本分析》为指导）<br>  ==&gt; （1）文本特征的选择方法（涉及TF-IDF、信息增益、卡方统计、互信息）<br>  ==&gt; （2）分类器设计（涉及朴素贝叶斯、SVM、KNN、决策树、逻辑回归等机器学习方法，并且熟悉使用sklearn库）</li>
</ul>
</li>
<li><p>软件（库）使用： </p>
<ul>
<li><div><div class="fold_hider"><div class="close hider_title">sklearn、Pytorch、NLTK、Spacy等几个常用NLTK库的对比</div></div><div class="fold">
<p><div style="border:0px;padding:3px; PADDING:0px; width:100%; height:400px; LINE-HEIGHT: 20px; OVERFLOW: auto; "><img src="/DIY/picture/【日志】几个NLP常用库的对比.png"><br></div></p>
<ul>
<li>NLTK（Python自然语言工具包）用于诸如标记化、词形还原、词干化、解析、POS标注等任务。该库具有几乎所有NLP任务的工具。</li>
<li>Spacy是NLTK的主要竞争对手。这两个库可用于相同的任务。</li>
<li>Scikit-learn为机器学习提供了一个大型库。此外还提供了用于文本预处理的工具。</li>
<li>Gensim是一个主题和向量空间建模、文档集合相似性的工具包。</li>
<li>Pattern库的一般任务是充当Web挖掘模块。因此，它仅支持自然语言处理（NLP）作为辅助任务。</li>
<li>Polyglot是自然语言处理（NLP）的另一个Python工具包。它不是很受欢迎，但也可以用于各种NLP任务。</li>
</ul>
<p><b>结论：</b><br>在文中，我们比较了几个流行的自然语言处理库的一些功能。虽然它们中的大多数都提供了重叠任务的工具，但有一些可以使用独特的方法来解决具体的问题。当然，目前NLP库中最受欢迎的软件包是NLTK和Spacy。他们在NLP领域是主要竞争对手。在我们看来，它们之间的区别在于解决问题的方法不同。NLTK更具学术性。用户可以使用它来尝试不同的方法和算法，将它们组合起来。相反，Spacy为每个问题提供了一个开箱即用的解决方案。用户不必考虑哪种方法更好：Spacy的编写者已经解决了这个问题。此外，Spacy的执行速度非常快（比NLTK快几倍）。但Spacy的一个缺点是所支持的语言数量有限。但其支持的语言数量将会一直增加。所以，我们认为Spacy在大多数情况下是用户的最佳选择，但如果用户想尝试一些特别的东西，可以使用NLTK。尽管这两个库很受欢迎，但还有许多不同的选项，NLP工具包的选择取决于用户必须解决的具体问题。</p>

</div></div></li>
</ul>
</li>
<li>面试常考内容（题库问答类应聘）??????????</li>
</ul>
<p><b id="em">涉及到的内容：</b></p>
<ul>
<li>数据结构与算法<ul>
<li>第一梯队：复杂度分析、链表、二叉树、栈、队列、二分查找、快速排序、归并排序、回溯法、动态规划、分治</li>
<li>第二梯队：堆、正则表达式、图 {DFS遍历、BFS遍历、最大（小）生成树算法（Kruskal、Prim）、最短路径（Dijkstra、Floyd）}</li>
<li>第三梯队：哈希、集合</li>
</ul>
</li>
<li>NLP传统模型<ul>
<li>HMM、EM、MaxEnt、CRF、LDA</li>
<li>N-gram</li>
</ul>
</li>
<li>NLP深度学习<ul>
<li>Word-Embedding（最为重要）及genism的使用、RNN、LSTM、GRU、Attention、Transformer、encode-decoder、递归神经网络</li>
</ul>
</li>
<li>ML<ul>
<li>SVM、朴素贝叶斯、决策树、K-means、KNN、提升算法、感知机、逻辑回归、SVD、PCA</li>
</ul>
</li>
</ul>
<p><b id="em">自己的优势</b>…<br><br><br><br></p>
<hr>






<h3 id="5-07-刷题-c-深度学习复习-t"><a href="#5-07-刷题-c-深度学习复习-t" class="headerlink" title="5.07　刷题(c) + 深度学习复习(t)"></a>5.07　刷题(c) + 深度学习复习(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer》刷了两道题和第三道题的理解与构思；</li>
<li>复习DNN的反向传递原理；</li>
<li>RNN的反向传递原理；</li>
<li>GRU和LSTM原理；</li>
<li>梯度爆炸与梯度消失深层理解；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>学习《剑指Offer》及类似题相关步骤：<ul>
<li>理解问题；</li>
<li>自己构思算法；</li>
<li>参考样本算法；</li>
<li>自己写代码；</li>
<li>参考样本代码；</li>
</ul>
</li>
<li>在实现算法时，现在自己最大的问题就是没有习惯将大的复杂的算法块分解为较小的功能单一的算法块，总习惯于一次性解决所有问题，将各种细节糅杂在一起，这样既不利于思考，也不利于实现；</li>
<li>增加代码的简洁性和可读性是必要的（比如变量命名、恰当的写注释），这点可以参考一些相关类的书籍；</li>
<li>对于python语言中内置的数据结构和算法应该熟悉其复杂度，也就是知道其内部实现原理（这点不一定必要），这样对于设计新的模块时计算自己的复杂度心里才有数；</li>
<li>算法没有固定的设计，任何算法，模式都有相当大的自由度，不要局限于已有的东西，开阔思路，“自定义”才是王道；</li>
<li>掌握其他的排序算法；</li>
<li>注意预防颈椎病；</li>
</ul>
<p><b id="em">待阅：</b><br>对近期的项目比较有帮助</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/63943609?utm_source=cn.ticktick.task&amp;utm_medium=social&amp;utm_oi=708757665970397184" target="_blank" rel="noopener">一些自然语言处理基本模型Demo</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/59190209?utm_source=cn.ticktick.task&amp;utm_medium=social&amp;utm_oi=708757665970397184" target="_blank" rel="noopener">[EMNLP18]用序列标注来进行成分句法分析</a><br><br><br><br><hr>





</li>
</ul>
<h3 id="5-08-pytorch熟悉使用-c"><a href="#5-08-pytorch熟悉使用-c" class="headerlink" title="5.08　pytorch熟悉使用(c)"></a>5.08　pytorch熟悉使用(c)</h3><p><b id="log">【行】</b></p>
<ul>
<li>熟悉pytorch的使用</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>利用好零碎时间，不要见缝插针地玩手机；</li>
<li>想要改变自己，必须改变自己的一些习惯；</li>
<li>学到一大类算法时，不必每个算法都去实现，实现其中的一个算法就行了，其他的都是触类旁通，简单修改细节就行了，这样既提高了理解力，也提高了实践能力；</li>
<li>每次看别人写的代码的时候应该注意先关心什么，再关心什么，每次重读代码的时候目的应该是不一样的，比如，第一遍是了解整个大概，第二遍，研究算法细节，第三遍，可以把不熟悉的语言函数或者其他没学过的东西拿来研究，做其他事情也应该这样，每次有每次的关注点，而不是大而全地所有都去关注，这样目的性更强，针对性更强，能学到更多的东西；</li>
</ul>
<p><b id="em">待优化：</b></p>
<ul>
<li>pytorch 的tensor grad梯度下降机制待考察<br><br><br><br><hr>





</li>
</ul>
<h3 id="5-09-pytorch熟悉使用-c"><a href="#5-09-pytorch熟悉使用-c" class="headerlink" title="5.09　pytorch熟悉使用(c)"></a>5.09　pytorch熟悉使用(c)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer》刷了一道题</li>
<li>熟悉pytorch的使用</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>尽量用显式的代码指出逻辑顺序，隐式的（比如顺序式+判断式）的算法会隐藏隐患；</li>
<li>数学公式恐惧感，刚开始看offer里面的讲解时，感觉地推公式完全就是数学化的东西，一瞬间就感觉很难，之后也没有仔细分析，我觉得这就是自己存在的对数学公式的恐惧感，但之后经过自己分析和研究发现这个公式的来源并不是凭空想象的，而是经过一步步思考一步步构建而来的，而每一步构建的过程并不难，所以要克服这种对数学的恐惧感；</li>
<li>没有头绪就烦躁，做些其他事情来转移注意力，结果浪费了更多的时间；</li>
<li>看到自己的进展才有兴趣，不清楚自己掌握了哪些东西，掌握的程度怎么样，好像什么都没有学到一样，只有在提起的时候可能会想起来，这个不利于积极的促进作用，应该学完每个知识点之后都有自己的总结和见解；<br><br><br><br><hr>






</li>
</ul>
<h3 id="5-10-刷题-c"><a href="#5-10-刷题-c" class="headerlink" title="5.10　刷题(c)"></a>5.10　刷题(c)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer》刷了两道题</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>看到一些好的讲座或者其他博文的时候，可以写写读后感，把自己的体会记录下来，也可以记录其中很有启发性的东西，思维方式，行业发展方向，开创性模型等等，学习学者在某些问题、模型上的理解，开拓自己的思维。比如在听了微软亚洲研究院刘铁岩的“形成机器学习研究的闭环”演讲后，自己就收益良多，一方面看到了很多他对于行业中某些现象，比如BERT、GPT等“大力出奇迹”（使用巨量计算资源进行训练）的理解，另一方面也看到了他们在机器学习上的创新，比如对偶学习、博弈学习、Coopertitve Learning、Lightweight Learning（与“大力出奇迹”相对）、改进的分布式学习等等。对于这些可能的优秀的方式方法、模型、理解角度等等应该系统地记录下来，供自己以后留出时间来学习；</li>
<li>在解决某些问题时，我们应该尽可能掌握所有已有的信息，自己有个缺点就是对于已有的信息，经常是记忆错误，或者因为略读忽略了重要信息，在不了解上下文环境的情况下就开始行动，结果导致大量的时间浪费、精力浪费还得不到结果；</li>
<li>动态规划是有固定的处理流程和使用条件的；</li>
<li>对于刷题，没有形成规范化的、系统化的总结，尽管自己有时候刷题对某个类型题进行了总结，但是在遇到相似题型时又总结了一些东西，但这两次总结的东西都没能合在一起，比较，分析，形成成套的解决方案，导致以后再遇到这种问题的时候自己依然凌乱没有思路，这就是有积累但很分散的结果，既不利于记忆，也不利于应用，比如在刷动态规划题的时候，做了几道相关题目，依然不能形成清晰的解题思路（实际情况是动态规划本身有适用的环境和解题规范，自己不知道）；</li>
<li>对自己的期待应该更高，而不是仅仅满足于现状，这样才能提高自己的能力；</li>
<li>更好地利用好的资源，比如github，它其实是一个很大的网站，自己只了解了一部分0内容；</li>
<li>改掉思考过程中不好的习惯，比如想写字，让思维更专注；</li>
<li>刷题方式，画出思路树图，统计图中所需要的变量，考虑变量表达形式；</li>
<li>习惯总结，不要用重复训练的方式来形成思路“感觉”，要用总结的方式减少重复性练习，重复性训练可以在总结之后作为巩固，但不是代替总结；<br><br><br><br><hr>






</li>
</ul>
<h3 id="5-12-随机思索"><a href="#5-12-随机思索" class="headerlink" title="5.12　随机思索"></a>5.12　随机思索</h3><p><b id="log">【悟】</b></p>
<ul>
<li>要习惯用记事本记录自己要做的事，定闹钟提醒，这样可以减少自己对这些东西的记忆，把精力集中做事；</li>
<li>还有这么多好玩的东西要学，还有时间去浪费吗，抓紧时间进步；</li>
<li>自己已经是起步比较晚了，如何加快步伐学习（并不是走捷径，而是更努力地抓紧时间学习）形成适合自己的高效的学习方法，选择正确的学习资源和路线需要仔细考虑清楚，我认为提高效率的方式之一是学习经典内容，减少对含金量低的内容的学习，通过反复琢磨经典的学习资源，从而快速掌握内容的核心，含金量低的内容包括各种网络来源不可靠的视频教程、博文等等；</li>
<li>不仅要学习与NLP相关的知识，计算机视觉、语音、机器学习也一样要学习，从整个AI行业的角度思考学习方向，这样才能全面而富有远见性，并且在其他领域的算法可能也能迁移到NLP中；</li>
<li>快速解决学习中不懂知识点的能力：<ul>
<li>在网络快速准确地搜索的能力；</li>
<li>询问同行业者得到问题答案的效率；</li>
</ul>
</li>
<li>在学习或者工作中如何持续集中精力：<ul>
<li>从生理上、时间安排上考虑，比如进行体育锻炼也许有助于提高精力集中或者精神兴奋性；</li>
<li>从思想上考虑，如何引起注意力等等；</li>
</ul>
</li>
</ul>
<p><b id="em">待阅：</b></p>
<ul>
<li style="list-style: none"><input type="checkbox" checked> <a href="https://mp.weixin.qq.com/s/m-BoQll0TxXS_Wns5HnDJg" target="_blank" rel="noopener">如何有效地做算法题</a></li>
<li style="list-style: none"><input type="checkbox"> <a href="https://mp.weixin.qq.com/s/0jf5m04uJFisFt9iks8AWw" target="_blank" rel="noopener">机器学习算法优缺点对比及选择</a><br><br><br><br><hr>







</li>
</ul>
<h3 id="5-14-刷题-c-MaxEnt复习-t-KNN复习和实现-c"><a href="#5-14-刷题-c-MaxEnt复习-t-KNN复习和实现-c" class="headerlink" title="5.14　刷题(c) + MaxEnt复习(t) + KNN复习和实现(c)"></a>5.14　刷题(c) + MaxEnt复习(t) + KNN复习和实现(c)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer》刷题两道</li>
<li>KNN复习并用sklearn实现训练</li>
<li>MaxEnt复习</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>坚持做算法题的目的：<ul>
<li>保持思维敏捷。非常重要，状态好才能保持对编程的热情。</li>
<li>对基础的数据结构、查找和排序保持熟练。能解决日常开发中的性能相关问题。</li>
<li>积累对问题域的探索。只有对问题域有足够的探索，才可能举一反三，迸发灵感。</li>
</ul>
</li>
<li>调整作息时间，避免困倦；<br><br><br><br><hr>






</li>
</ul>
<h3 id="5-15-刷题-c-HMM、CRF、MaxEnt复习-t-决策树复习-t"><a href="#5-15-刷题-c-HMM、CRF、MaxEnt复习-t-决策树复习-t" class="headerlink" title="5.15　刷题(c) + HMM、CRF、MaxEnt复习(t) + 决策树复习(t)"></a>5.15　刷题(c) + HMM、CRF、MaxEnt复习(t) + 决策树复习(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer&gt;刷题一道</li>
<li>HMM、CRF、MaxEnt复习</li>
<li>决策树复习</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>编码习惯：代码必要的注解：<ul>
<li>各代码块的注解；</li>
<li>对于含义模糊的变量的解释；</li>
</ul>
</li>
<li>刷第二遍《剑指Offer》，总结题型和规律；</li>
<li>从刷题中的启示，反复复习经典的内容，有利于更深入的理解和应用；</li>
<li>每个经典算法模型并不是从吴恩达机器学习课程中学的那么简单，深入下去有很多细节性的内容；</li>
<li>随时提醒自己是为了找工作而复习，所以复习的时候针对性复习，该忽略的地方就忽略；</li>
</ul>
<p><b id="em">遗问：</b></p>
<ul>
<li>决策树一节在以后有时间的时候结合《机器学习》一书进行深入学习，决策树一节在以后有时间的时候结合《机器学习》一书进行深入学习；</li>
<li>knn的kd搜索树原理</li>
<li>HMM的训练方法和后向算法原理没有弄明白，训练方法在《自然语言处理综论》P160页红色标记处不懂；</li>
<li>CRF的训练方法没有仔细研究；<br><br><br><br><hr>






</li>
</ul>
<h3 id="5-16-复习刷过的题-c-Pytorch使用建模-c-AdaBoost复习-t"><a href="#5-16-复习刷过的题-c-Pytorch使用建模-c-AdaBoost复习-t" class="headerlink" title="5.16　复习刷过的题(c) + Pytorch使用建模(c) + AdaBoost复习(t)"></a>5.16　复习刷过的题(c) + Pytorch使用建模(c) + AdaBoost复习(t)</h3><p><b id="log">【行】</b></p>
<ul>
<li>复习《剑指Offer》刷过的题两道</li>
<li>使用Pytorch建立词向量和N元语法模型；</li>
<li>复习AdaBoost模型（简单复习）</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>在复习刷题中使用计时；</li>
<li>在设计算法前考虑测试用例；</li>
<li>运用某种已掌握的算法时要注意在其他的环境中，哪些地方是不一样的，比如边界值在原有算法中计入，但是在新环境中需要计入，这些都需要仔细考虑；</li>
<li>pycharm的断点使用练习；</li>
<li>读书笔记的勾、画总结应该尽量简短，精炼；</li>
<li>在以后深入研究机器学习时，使用《统计方法学习》一书时，可以把每章后面的参考文献拿来仔细研究，看看最本质的思想；</li>
<li>简历里面添加英文阅读能力；</li>
<li>将查询过的英语单词放入有道单词本，经常复习；</li>
<li>了解pytorch的框架对使用pytorch很有帮助；</li>
<li>pytorch中的各种参数使用形式要记，不然有时候出错在哪根据错误信息自己完全不知道；<br><br><br><br><hr>





</li>
</ul>
<h3 id="5-17-复习刷过的题-c-Pytorch使用建模-c"><a href="#5-17-复习刷过的题-c-Pytorch使用建模-c" class="headerlink" title="5.17　复习刷过的题(c) + Pytorch使用建模(c)"></a>5.17　复习刷过的题(c) + Pytorch使用建模(c)</h3><p><b id="log">【行】</b></p>
<ul>
<li>复习《剑指Offer》刷过的题两道和二叉树的各种遍历</li>
<li>使用Pytorch实现LSTM和使用LSTM进行词性标记</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li><a href="https://zybuluo.com/hanbingtao/note/581764" target="_blank" rel="noopener">lstm、rnn等的纯手工实现</a></li>
<li>官方文档是最权威的查询资料；</li>
<li>深入了解算法的本质原理才能活用，比如以下简单的模型有多层的理解：</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:left">kNN 的花式用法<font size="1em"><a href="https://zhuanlan.zhihu.com/p/62450795" target="_blank" rel="noopener">(link)</a></font></th>
<th style="text-align:left">EM算法的九层境界：​Hinton和Jordan理解的EM算法<font size="1em"><a href="https://mp.weixin.qq.com/s/NbM4sY93kaG5qshzgZzZIQ?" target="_blank" rel="noopener">(link)</a></font></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">① 分类</td>
<td style="text-align:left">① EM就是 E + M</td>
</tr>
<tr>
<td style="text-align:left">② 回归</td>
<td style="text-align:left">② EM 是一种局部下限构造</td>
</tr>
<tr>
<td style="text-align:left">③ One-class识别</td>
<td style="text-align:left">③ K-Means是一种Hard EM算法</td>
</tr>
<tr>
<td style="text-align:left">④ 搭配核函数</td>
<td style="text-align:left">④ 从EM 到 广义EM</td>
</tr>
<tr>
<td style="text-align:left">⑤ 搭配空间分割技术</td>
<td style="text-align:left">⑤ 广义EM的一个特例是VBEM</td>
</tr>
<tr>
<td style="text-align:left">⑥ 超球体空间分割</td>
<td style="text-align:left">⑥ 广义EM的另一个特例是WS算法</td>
</tr>
<tr>
<td style="text-align:left">⑦ 冗余样本剔除</td>
<td style="text-align:left">⑦ 广义EM的再一个特例是Gibbs抽样算法</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">⑧ WS算法是VAE和GAN组合的简化版</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">⑨ KL距离的统一</td>
</tr>
</tbody>
</table>
<p>简单的招数练到极致就是绝招<br>关键词：深入立即基础算法+结合其他优异性能的算法</p>
<p><br><br><br></p>
<hr>






<h3 id="5-18-2017-2019-ACL各子领域论文统计"><a href="#5-18-2017-2019-ACL各子领域论文统计" class="headerlink" title="5.18　2017-2019 ACL各子领域论文统计"></a>5.18　2017-2019 ACL各子领域论文统计</h3><p><b id="log">【行】</b><br><img src="/DIY/picture/【日志】02.png" alt="link"></p>
<p><img src="/DIY/picture/【日志】01.png" alt="link"></p>
<p><a href="https://www.bilibili.com/video/av54045631?from=search&amp;seid=1071446162748506589" target="_blank" rel="noopener">刘知远关于2019ACL投稿情况的分析报告</a><br><br><br><br></p>
<hr>






<h3 id="5-23-休息"><a href="#5-23-休息" class="headerlink" title="5.23　休息"></a>5.23　休息</h3><p><b id="log">【悟】</b></p>
<ul>
<li>清楚做每件事的目的，提高效率；</li>
<li>从ACL的论文领域可以看出，NLP的基础问题研究并没有结束；</li>
<li>提高单位时间的效率，增加学习时间；</li>
<li>学习的时候对于没有学完的东西，应该记录学到什么层次了，学到什么地方了；<br><br><br><br><hr>






</li>
</ul>
<h3 id="5-27-Think-of-NLP-as-a-Life"><a href="#5-27-Think-of-NLP-as-a-Life" class="headerlink" title="5.27　Think of NLP as a Life . . ."></a>5.27　Think of NLP as a Life . . .</h3><p><b id="log">【悟】</b></p>
<ul>
<li>可以从<b><a href="http://csrankings.org/#/index?nlp&amp;world" target="_blank" rel="noopener">CSRanking</a></b>中<b id="em">查询NLP领域各个大学的排名，并且，更重要的是可以查到每个大学发文的排名和个人网站</b>；</li>
<li>可以根据NLP关键字查询最相关的学者，格式如：“label:machine_learning”、“label:information_extraction”，这是根据标签进行的查询，注意下划线是必要的，根据上述格式搜索出的学者有些是给出了学者的个人网页的。如果没有“label:”这个，搜索的就是论文而不是学者；<br><br><br><br><hr>






</li>
</ul>
<h3 id="5-28-刷题"><a href="#5-28-刷题" class="headerlink" title="5.28　刷题"></a>5.28　刷题</h3><p><b id="log">【行】</b></p>
<ul>
<li>《剑指Offer》刷题</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>与NLP相关的网站基本上都是英文、外国的为主，所以提高英文能力是非常必要的，虽然国内也有相关网站，但很少有高质量的，并且国内学术风气比较浮躁且利益性太重，不适合拓展眼界和培养科研品味；</li>
<li>阅读最前沿科技论文的最基本要求是<b id="em">基础要扎实</b>。刚开始阅读科技文献的时候，不要选难度太大的，最好选经典的文献，一方面对基础原理讲述比较清楚，另一方面有很多人阅读过这篇文献，做了不少笔记，这些笔记可以帮助自己理解，因为刚开始肯定有许多地方是不能够理解的，这样慢慢培养阅读能力；</li>
<li>在使用super关键字的时候，以前就没有学得很透彻，反过来又学一遍，联想到还有python基础相关的其他关键字以及扩展开来python的更多基础内容并没有学得更扎实，这在方法上属于举一反三能力，在知识的掌握上属于未系统性的学习；</li>
<li>再在在学习吴恩达的视频的时候，很多细节性的内容和扩展性的内容都没有讲到（比如讲RNN的变形的时候，就有多对多、多对一、一对多、一对一四种模型，但自己并没有花时间去钻研每种模型的使用场景和条件），包括在学习《自然语言处理综论》中的内容，虽然已经够详细了，但还是有些内容没有讲详细，需要自己去扩充学习，这些都导致了基础内容掌握并不扎实；</li>
<li>基础内容第一遍不一定掌握得很全面，所以在以后的学习中，看到有不同的理解时（前提是记得以前所学的知识点），需要再次进行学习，深度挖掘，再次学习的重点是深层次的理解，而并非浅层次的因为遗忘而再次复习；</li>
<li>间歇性地停止学习产生的可能原因：<ul>
<li>因为恐惧感，感到要学的东西太多了，而时间又不够，还不一定学得好，干脆就停止去干其他事了；</li>
<li>看不到有什么实际应用，能产生什么效果，产生厌倦感；</li>
</ul>
</li>
<li>在平时浏览网页的时候，经常会看到很多自己不知道但又需要掌握的内容，然后就会产生焦虑情绪，这也是可能是导致自己间歇性停止学习的原因之一，其实本质怕太多反而产生厌烦情绪，然后停止学习，白白耽误了更多时间，学习其实怕的不是慢，而是停止，其实解决办法也很简单，自己开始一点一点的学习，并记录学习进度，这样看到自己的进步就能逐渐淡化这种“恐惧”心理。对于看到的不懂的内容，可以将其记录在某个记事本上，平时利用零散时间进行学习，其实这种零散时间很多，真到了零散时间了，又不愿意看，害怕没有做笔记或者不能演算进而不能掌握得很好；</li>
<li>学会用简短精炼的语句讲述自己所学的，所感的内容，尽量不要大段大段引用别人的话；<br><br><br><br><hr>






</li>
</ul>
<h3 id="5-30-找工作计划"><a href="#5-30-找工作计划" class="headerlink" title="5.30　找工作计划"></a>5.30　找工作计划</h3><p><b id="em">主要考虑一下几点：</b></p>
<ul>
<li>以<b id="em">尽快找到工作为目的</b>，不能再完全准备好了再找了，要边找边学习；</li>
<li>对准备找工作过程中学习的每个知识点都尽量掌握完整；</li>
<li>以经典资料为学习内容，尽量避免接触零散的、不确定来源可靠性的资料，不要被网络上各种各样的资料看花眼；<br><br><br><br><hr>






</li>
</ul>
<h2 id="2019年6月"><a href="#2019年6月" class="headerlink" title="2019年6月"></a>2019年6月</h2><h3 id="6-25-反思状态、制定计划"><a href="#6-25-反思状态、制定计划" class="headerlink" title="6.25　反思状态、制定计划"></a>6.25　反思状态、制定计划</h3><p><b id="log">【悟】</b></p>
<ul>
<li>很多博文下面的评论有时也是经典的理解方向；</li>
<li>youtube具有更丰富的视频资源；<br><br><br><br><hr>





</li>
</ul>
<h3 id="6-26-刷题-分词项目-概率学习"><a href="#6-26-刷题-分词项目-概率学习" class="headerlink" title="6.26　刷题 + 分词项目 + 概率学习"></a>6.26　刷题 + 分词项目 + 概率学习</h3><p><b id="log">【行】</b></p>
<ul>
<li>调试pycharm，设置界面；</li>
<li>分词项目总结查询，准备实施；</li>
<li>学习叶丙成的《机率》</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>在学习台湾教授叶丙成的课程中体会到，一方面结合实际例子理解基础概念，这是非常重要的，但另一方面，自己已经是第三遍学习概率论了，已经有了一定的基础，不能再只停留于表面，如何结合已会的概念深度系统地学习是个值得思考的问题，在叶丙成的这门课中，使用的是《Probability and  Statistics》这本书，这次的学习是否应该结合这本书开始深度研究概率论呢这个值得思考；<br><br><br><br><hr>




</li>
</ul>
<h3 id="6-27-刷题-感知机学习-概率学习"><a href="#6-27-刷题-感知机学习-概率学习" class="headerlink" title="6.27　刷题 + 感知机学习 + 概率学习"></a>6.27　刷题 + 感知机学习 + 概率学习</h3><p><b id="log">【行】</b></p>
<ul>
<li>感知机的深入学习；</li>
<li>感知机的代码编程；</li>
<li>各种代码操作的温故；</li>
<li>叶丙成《机率》的学习；</li>
<li>林轩田的学习基石；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>现阶段主要复习要用到的数学，而不是系统巩固旧知识；</li>
<li>今天在学习过程中的主动思考的行为减少了，这要注意；</li>
<li>养成良好的写代码习惯，代码写成模块（函数、类别、包）等形式，这是从下载的github上关于《统计学习方法》代码实现上对比自己编写的感知机代码得出的结论；</li>
<li>刷题优先安排时间，今天在时间安排上存在问题；</li>
<li>学会写明日事情安排，并且在第二天晚上写日志的时候检查完成程度，目的在于提高规划能力和执行力；</li>
<li>在学习林轩田的视频的时候，发现其实并不那么简单，需要一定的机器学习基础和数学知识，但他从另外一些角度展示了机器学习的理解，这与吴恩达的入门级知识不同，与李航的《统计学习方法》数学方法为主也不同，从中可以看出，选一些经典的资料有助于自己开阔眼界，增加对知识的理解深度；</li>
</ul>
<p><b id="em">遗问：</b></p>
<ul>
<li>PLA为甚么称作在线学习，它也需要检验一遍所有样本，并且在不能正确划分样本的情况下还需要迭代；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>刷题；</li>
<li>完成对感知机的理解，包括为什么有限次数后会收敛，但对于对偶的问题（相对复杂），先不用弄清楚；</li>
<li>完成对分词的归纳总结，并且设定好需要写的项目；</li>
<li>零星时间可以学习叶丙成的《机率》<br><br><br><br><hr>






</li>
</ul>
<h3 id="6-28-刷题-感知机学习-复习中文分词系统"><a href="#6-28-刷题-感知机学习-复习中文分词系统" class="headerlink" title="6.28　刷题 + 感知机学习 + 复习中文分词系统"></a>6.28　刷题 + 感知机学习 + 复习中文分词系统</h3><p><b id="log">【行】</b></p>
<ul>
<li>刷题，并复习python相关知识；</li>
<li>理解感知机的1/||w||为什么可以在损失函数中省略；</li>
<li>明白了损失函数的批量梯度下降和随机梯度下降原理；</li>
<li>理解了什么叫结构化的感知机和结构化的SVM；</li>
<li>理解基于无标注数据的半指导特征分词方法，卡方分布，边界熵；</li>
<li>理解基于新词识别的切分方法；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>《python3学习笔记》一书的学习放在后面进行学习，在应聘时如果觉得python没有必要先深入，马上用到的知识可以先学习，没有使用急切性的就放在工作以后再学习，如果在面试时觉得有必要，就边学边找，这种类型的书籍完全可以利用零星时间进行学习，对于《数据结构与算法 Python语言描述》这类书才需要花整块时间研究，这是按照对于工作中重要性、核心性来分类的；</li>
<li>异常的python处理；</li>
<li>不要有标准答案的思想；</li>
</ul>
<p><b id="em">遗问：</b></p>
<ul>
<li>对于感知机的次数收敛问题的证明不是很明白；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>刷题；</li>
<li>复习深度学习和条件随机场的基本内容，为项目做准备；</li>
<li>熟悉中文分词哪些软件比较好用；<br><br><br><br><hr>





</li>
</ul>
<h3 id="6-29-休整"><a href="#6-29-休整" class="headerlink" title="6.29　休整"></a>6.29　休整</h3><p><b id="log">【行】</b></p>
<ul>
<li>简单复习了学过的叶丙成的《机率》；</li>
<li>打印线性代数、机器学习基石、机器学习技法的课件；</li>
<li>任务完成度：0；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>学习数学基础需要大量练习和测试，而MIT官网上就有，尽量不要看答案，就像叶丙成说的，逼一逼自己，锻炼下自己的韧性；</li>
<li>对于未完成的任务，必须加到明日安排中，观察任务的执行能力；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>学习《机率》；</li>
<li>刷题；</li>
<li>复习深度学习和条件随机场的基本内容，为项目做准备；</li>
<li>熟悉中文分词哪些软件比较好用；<br><br><br><br><hr>






</li>
</ul>
<h3 id="6-30-学习《机率》"><a href="#6-30-学习《机率》" class="headerlink" title="6.30　学习《机率》"></a>6.30　学习《机率》</h3><p><b id="log">【行】</b></p>
<ul>
<li>学习《机率》，基本上复习有1/2的内容；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>叶丙成的课非常基础，但是缺乏一些系统性的练习，结合《Probability and Statistics(Fourth)》这本书进行系统性的学习和练习比较好，它们之间相互补充，叶丙成的课提供基础讲解，这本书提供练习、系统化和深化知识结构；</li>
<li>注意自己一些习惯，比如把某些事留到以后做的习惯，这个要思考是否真的有必要现在不做；</li>
<li>有些知识点明明没有掌握得很好却要继续往下学，这个习惯不好；</li>
<li>叶丙成很好的观点是学以致用，学习了就拿来使用，深化对概念的理解；</li>
<li>面对未知问题需要的东西：再尝试的勇气和信心，多样化的解题能力；</li>
<li>除了专业之外应该有自己的爱好和广泛的知识；</li>
<li>任务完成度：0；</li>
</ul>
<p><b id="em">遗问：</b></p>
<ul>
<li>《机率》中进击的钞票一题没做；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>学习《机率》；</li>
<li>刷题；</li>
<li>复习深度学习和条件随机场的基本内容，为项目做准备；</li>
<li>熟悉中文分词哪些软件比较好用；<br><br><br><br><hr>




</li>
</ul>
<h2 id="2019年7月"><a href="#2019年7月" class="headerlink" title="2019年7月"></a>2019年7月</h2><h3 id="7-01-学习《机率》"><a href="#7-01-学习《机率》" class="headerlink" title="7.01　学习《机率》"></a>7.01　学习《机率》</h3><p><b id="log">【行】</b></p>
<ul>
<li>学习《机率》，基本上学习有1/4的内容；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>反复对比学习，忘了就重新翻，重新算，巩固加深记忆；</li>
<li>任务完成度：0；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>学习《机率》；</li>
<li>刷题；</li>
<li>复习深度学习和条件随机场的基本内容，为项目做准备；</li>
<li>熟悉中文分词哪些软件比较好用；<br><br><br><br><hr>





</li>
</ul>
<h3 id="7-02-学完《机率》"><a href="#7-02-学完《机率》" class="headerlink" title="7.02　学完《机率》"></a>7.02　学完《机率》</h3><p><b id="log">【行】</b></p>
<ul>
<li style="list-style: none"><input type="checkbox" checked> 学习完叶丙成的《机率》；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>在学习完《机率》之后的一些总结：<ul>
<li>学习理论知识一定要以实际的例子进行练习和应用，不仅仅要在学习理论之后要结合实例进行练习巩固，在学习理论的过程中结合例子也能够提高理解的效率和深度，特别的，在学习理论之前也可以用例子作为引导，比如在学MGF之前，通过“红娘转换场景促成恋爱”的例子中就首先用例子进行了过程的形象描述，这为后面理解MGF整个流程打下了框架基础，但这一点是针对于给别人讲课或者别人通过例子来引导我们的时候；</li>
<li>多做习题，它的作用有以下几个：<ul>
<li>巩固所学的东西，深刻理解理论；</li>
<li>纠正错误的理解，理解不正确的在做题的过程中会出现与预期不同的结果；</li>
<li>学以致用，提高建模能力；</li>
</ul>
</li>
<li>学习的时候要对比学习，忘了就重新翻查，重新计算，这样能提高对知识点的记忆；</li>
<li>在学习中的主动性思考非常重要；</li>
<li>基础理论一定要学好，这是走向复杂，解决更多难题的根本，比如在阅读英文书籍的时候，往往可能因为语言问题而理解不了某些概念，但是如果某些基础概念比较清楚，就会促进自己从公式的形式上结合自己基础知识进行猜测学习，然后就是通过例题来进行验证；</li>
<li>打印课件的时候要增加页数和目录，方便翻阅查找，在每章的结束位置插入空白页方便自己进行相应的知识整理；</li>
<li>缺少了对每一章节的系统整理，各种知识在头脑里比较零碎；</li>
<li>前面学的知识学好了再往后面学，不然可能会弄得糊里糊涂的，反而耽误了更多时间；</li>
<li>对于书上没有讲或者讲得不清楚的概念需要自己在网站查，因为它涉及的知识点可能还包括其他内容，了解它们对于了解知识点、拓宽知识面有很大帮助，比如X的$n^{th}$moment是什么意思刚开始就没怎么弄懂，以为就是n次方的意思，结果实际上它表示的是数学中“矩”的概念，而n阶的矩表示不同的含义；</li>
<li>许多概念依然不是很熟，概念的区分依旧存在问题，需要后续继续翻阅理解和记忆，比如多项式组合；</li>
<li>某些知识点的推导很重要，有时候它能启发其他知识点的推导，比如连续函数中密度函数的推导；</li>
<li><b id="line">《机率》这门课只是讲了一些比较基础性的、必要的知识点，很多其他更全面、更深入的知识需要结合其他相关书籍进行深化；</b></li>
<li>有些证明依然没有完成，比如各种函数分布的期望、方差公式推导，各种函数的MGF推导，有些题也没有做，比如“进击的钞票”；</li>
</ul>
</li>
<li>对于长期计划的东西，比如数学，应该每天积累一点，不要等有整块时间了才来学习，这样的整块时间往往很少；</li>
<li>要敢于突破固有习惯生活学习，比如在时间分配方面，总是担心如果把时间花在了其他不是学习的方面就会得不偿失，把做其他事情的时间压缩在很小的范围内，其实做其他事情，比如运动，练琴、看书等等，也许会促进学习的进行，因为大脑注意力得到了转移，没有长时间做一件事情而感觉乏味、疲劳；<br><br><br><br><hr>






</li>
</ul>
<h3 id="7-03-刷题-《统计学习方法》复习"><a href="#7-03-刷题-《统计学习方法》复习" class="headerlink" title="7.03　刷题 + 《统计学习方法》复习"></a>7.03　刷题 + 《统计学习方法》复习</h3><p><b id="log">【行】</b></p>
<ul>
<li>刷题；</li>
<li>深入阅读《统计学习方法》第一章和第六章；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>给事情分急迫性和重要性分等级；</li>
<li>学着克服自己的不好的习惯；</li>
<li>在看书学习的过程中，纸上应该记录什么东西；</li>
<li>看书学习的时候经常不仔细，某些符号的意思想当然地认为或者直接忽略了该符号，导致理解出现偏差，这个习惯要改；</li>
<li>《机率》的学习和复习使得在重新阅读《统计学习方法》时很多原来不清楚的概念更容易理解了，并且对ML的学习理解更深入了，所以基础性知识非常重要；</li>
<li>在学习《统计学习方法》的过程中，看到有在复习了监督学习的基本框架后，再回顾各种模型，发现都是大同小异，所以框架的根本思想掌握非常重要，它就像一个指导，在学习新的类似的模型时指导自己套用学习、对比；</li>
<li>如何有效地在书上做笔记，可以根据自己回看笔记，哪些是重点通过自己的勾画或者描述讲清楚了，哪些是没有必要勾画的以免把书弄得太乱；</li>
<li>记录在学习过程中遇到的暂时不能够解决或者暂时没时间解决的问题；</li>
<li>任务完成度：0.2;</li>
</ul>
<p><b id="em">遗问：</b></p>
<ul>
<li>《统计学习方法》P87-92</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>学习《线性代数》1小时；</li>
<li>刷题；</li>
<li>复习深度学习和条件随机场的基本内容，为项目做准备；</li>
<li>熟悉中文分词哪些软件比较好用；<br><br><br><br><hr>






</li>
</ul>
<h3 id="7-04-刷题-《统计学习方法》复习"><a href="#7-04-刷题-《统计学习方法》复习" class="headerlink" title="7.04　刷题 + 《统计学习方法》复习"></a>7.04　刷题 + 《统计学习方法》复习</h3><p><b id="log">【行】</b></p>
<ul>
<li>刷题；</li>
<li>复习《统计学习方法》中HMM和CRF章节；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>MIT《线性代数》的讲课中有很多引导性思考，这可以培养自己的自主探索能力；</li>
<li>即使你能跟上视频的速度，你也需要留点时间给自己思考，因为学习往往就在此刻发生；</li>
<li>写好的《剑指offer》要在牛客网或者leetcode上刷，看是否能通过，因为自己有很多地方可能没有考虑到，这也是锻炼代码鲁棒性的方法；</li>
<li>对于概念的理解、字母的含义、公式的推导，需要结合各个已有的信息，而自己经常忘记甚至跟没没注意书上写的；</li>
<li>在刷题的时候，先是白板编程，再是实际编程，但是在实际编程的时候有些地方又经常修改，这应该是在白板编程的时候注意这些问题；</li>
<li>任务完成度：0.5；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>学习《线性代数》1小时；</li>
<li>刷题；</li>
<li>复习深度学习和条件随机场的基本内容，为项目做准备；</li>
<li>熟悉中文分词哪些软件比较好用；<br><br><br><br><hr>






</li>
</ul>
<h3 id="7-05-刷题-deeplearning复习"><a href="#7-05-刷题-deeplearning复习" class="headerlink" title="7.05　刷题 + deeplearning复习"></a>7.05　刷题 + deeplearning复习</h3><p><b id="log">【行】</b></p>
<ul>
<li>刷题；</li>
<li>复习吴恩达的deeplearning；</li>
<li>任务完成度：0.7；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>学习MIT的《线性代数》，感觉虽然能够提供一些理解的新角度，但是，课堂上讲的并不系统，甚至有时候比较错乱，想要系统一些需要额外地看《Introduction to Linear Algebra》全英文的书籍，并且可能是英文书籍有时候概念并不能理解，感觉学下来要花很多的时间和精力，然而，回到最初的学习目的AI，线性代数可能提供的只是在计算方面的内容，也许偶尔会有应用原理性的知识点，但是这也是相当有限的，而对于AI来讲，更重的是概率论与数理统计，所以我认为在学习这些不是非常重要的领域时，不要太过追求完美、追求系统性的学习而牺牲了大量的时间，这些知识点也许只要会用就行，微积分其实也一样，应该把大量的时间花在概率统计、机器学习这些非常关键的内容上；</li>
<li>代码顺序上的调整也许可以省略很多不必要的代码；</li>
<li>在设计代码的时候，思路应该尽量简洁，复杂的设计思路会带来其他的一些问题，不仅仅是繁杂，比如在刷《合并两个排序的链表》的题时，就在两个链表的有相同值的处理时选择了将&lt;=的情况合并处理，分开处理时情况已经比较复杂，但是在实现的时候，隐藏的链表指针的变动产生了更为复杂的情况，结果导致写代码的时间过长，并且程序还运行有问题，其实这也是观察没有到位的结果，其实在设计的时候&lt;=的情况完全可以分开处理，并且后续发生的实际情况与原有思路一致，修正思路之后一遍就把代码写了出来，所以设计时观察规律，简单的试算几步，也许情况就没有那么复杂了；</li>
<li>写代码时，即使想不出来最聪明的方案，也不要忘了最笨的方法；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>学习《线性代数》1小时；</li>
<li>刷题；</li>
<li>复习深度学习基本内容，为项目做准备；</li>
<li>熟悉中文分词哪些软件比较好用；<br><br><br><br><hr>






</li>
</ul>
<h3 id="7-06-休整-刷题-deeplearning复习"><a href="#7-06-休整-刷题-deeplearning复习" class="headerlink" title="7.06　休整 + 刷题 + deeplearning复习"></a>7.06　休整 + 刷题 + deeplearning复习</h3><p><b id="log">【行】</b></p>
<ul>
<li>学习《线性代数》1小时；</li>
<li>刷题</li>
<li>deeplearning简单复习</li>
<li>任务完成度：0.5；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>学习《线性代数》1小时；</li>
<li>刷题；</li>
<li>复习深度学习基本内容，为项目做准备；</li>
<li>熟悉中文分词哪些软件比较好用；<br><br><br><br><hr>





</li>
</ul>
<h3 id="7-07-休整-线代-deeplearning复习"><a href="#7-07-休整-线代-deeplearning复习" class="headerlink" title="7.07　休整 + 线代 + deeplearning复习"></a>7.07　休整 + 线代 + deeplearning复习</h3><p><b id="log">【行】</b></p>
<ul>
<li>学习《线性代数》1小时；</li>
<li>deeplearning复习；</li>
<li>任务完成度：0.2；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>马辉的“线性代数”中节奏比较快，缺乏更多的练习，所以在每开始一天的学习的时候要复习前面的内容，加深对学过东西的理解，必要时加入一些其他例子尽心运算，这个例子也告诉我要根据不同视频的教学特点来调整自己的学习方式；</li>
<li>在学习“线性代数”的时候难度逐级递增，理解的时间花费更长，所以在以后学习安排时间的时候，要这样考虑时间长短，而不是将所有章节的学习时间作为均匀分布；</li>
<li>在今天的学习中我感觉到，国外的教学方式和国内教学方式与人才培养的不同，国内课程纯概念居多，定理居多，计算居多，而对于本质上的东西讲解较少，国外相反，对于本质上的讲解，推导、理解、应用居多，而要应用于实际，对于概念本质理解是必不可少的，相对于国外，要掌握国内的讲解方式所讲的东西的本质，一种要求是需要较高悟性的人，这就在无形中提高了对于学习者的要求（所谓的高智商），另一种要求就是多练习，达到的只是“熟练度”的提升或者理解的那么一点点提升，其实这也不仅仅局限于国内教学和国外教学，国内也就讲解的很好的，但是方式基本都类似于国外的方法，所以从学习方式的本质上来说，对于基础概念本质上的理解才是应用知识去解决世界问题的根本；</li>
<li>关于刷题的方法，一方面要动手实践写代码，另一方面，需要没事的时候就看看其他题，看看各种解题思路，减少遇到不会题的可能性；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>学习《线性代数》1小时；</li>
<li>刷题；</li>
<li>复习深度学习基本内容，为项目做准备；</li>
<li>熟悉中文分词哪些软件比较好用；<br><br><br><br><hr>





</li>
</ul>
<h3 id="7-08-线代-deeplearning复习"><a href="#7-08-线代-deeplearning复习" class="headerlink" title="7.08　线代 + deeplearning复习"></a>7.08　线代 + deeplearning复习</h3><p><b id="log">【行】</b></p>
<ul>
<li>学习线性代数3小时；</li>
<li>deeplearning；</li>
<li>刷题；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>线性代数时间安排过多，目前主要是花在准备面试和夯实基础上面；</li>
<li>《线性代数》分配时间不合理，在规定时间的时候，应该把理解的时间也投入在内，而不是以视频的时间多少为主线，不然这个时间就不好控制了，学完一个视频，理解了其中的内容，然后再看下一个视频，这样时间上的出入不会太大；</li>
<li>畏惧问题的心理才是效率、解题的关键，这是长期养成的，应该思考如何改变；</li>
<li>学习知识时就应该相对系统地学习，零散地、片面地反复学习浪费了更多的时间，从基础开始，一步一步来，一定会比零散的要强；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>学习《线性代数》1小时；</li>
<li>刷题；</li>
<li>复习深度学习基本内容，为项目做准备；</li>
<li>熟悉中文分词哪些软件比较好用；<br><br><br><br><hr>






</li>
</ul>
<h3 id="7-09-休整-线代"><a href="#7-09-休整-线代" class="headerlink" title="7.09　休整 + 线代"></a>7.09　休整 + 线代</h3><p><b id="log">【行】</b></p>
<ul>
<li>学习《线性代数》1小时；</li>
<li>任务完成度：0.1；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>补数学基础知识的时候，有些公理应该自己证，这样可以提高自己的数学能力，但是现在面临找工作，所以先不把时间花在证明上面，以后有时间再证明，但有一点，对于线性代数这类计算类学科，真的有必要花时间证明定理吗；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>刷题；</li>
<li>复习深度学习基本内容，为项目做准备；</li>
<li>熟悉中文分词哪些软件比较好用；<br><br><br><br><hr>






</li>
</ul>
<h3 id="7-10-制定学习计划"><a href="#7-10-制定学习计划" class="headerlink" title="7.10　制定学习计划"></a>7.10　制定学习计划</h3><p><b id="log">【行】</b></p>
<ul>
<li>任务完成度：0；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li><b id="line">缺乏自制力，应该学习如何提高自制力；</b></li>
<li>补数学基础知识的时候，有些公理应该自己证，这样可以提高自己的数学能力，但是现在面临找工作，所以先不把时间花在证明上面，以后有时间再证明，但有一点，对于线性代数这类计算类学科，真的有必要花时间证明定理吗；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>刷题；</li>
<li>写序列标注项目；</li>
<li>熟悉中文分词哪些软件比较好用；<br><br><br><br><hr>




</li>
</ul>
<h3 id="7-15-Nothing"><a href="#7-15-Nothing" class="headerlink" title="7.15　Nothing"></a>7.15　Nothing</h3><p><b id="log">【行】</b></p>
<ul>
<li>任务完成度：0；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>自信力不够；</li>
<li>以点带面，前提是对点的各个细节和扩展都比较透彻了；<br><br><br><br><hr>



</li>
</ul>
<h3 id="7-16-lt-再见，过去-gt"><a href="#7-16-lt-再见，过去-gt" class="headerlink" title="7.16　&lt; 再见，过去 &gt;"></a>7.16　&lt; 再见，过去 &gt;</h3><ul>
<li>分解目标；</li>
<li>充满自信；</li>
<li>竭尽全力；</li>
<li>强迫自己改变；</li>
<li>给自己加油，不断告诉自己别放弃；</li>
<li>适当给自己压力；</li>
<li>到底结果会怎样？</li>
<li>自己潜力有多大？</li>
<li>不要骗自己；</li>
<li>坚持，不知道得到的是什么，但是，不坚持，知道自己会后悔；</li>
<li>让这个世界更美好；</li>
<li>你， 要怎么样的度过这一生？</li>
<li>别再浪费时间， 哪怕一分一秒；</li>
<li>懒惰久了，努力一下就以为字拼尽了全力；</li>
<li>凡是不能将我毁灭的，必将使我变得更强；</li>
<li>给自己坚毅的眼神；</li>
<li>做些疯狂的事；</li>
<li>勇敢去做，勇敢去尝试，活出精彩；<br><br><br><br><hr>



</li>
</ul>
<h3 id="7-17-刷题-Pytorch复习"><a href="#7-17-刷题-Pytorch复习" class="headerlink" title="7.17　刷题 + Pytorch复习"></a>7.17　刷题 + Pytorch复习</h3><p><b id="log">【行】</b></p>
<ul>
<li>刷题；</li>
<li>复习数据结构：树；</li>
<li>Pytorch基础编程练习；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>刷题；</li>
<li>完成用深度学习进行序列标注；</li>
<li>总结分词系统；<br><br><br><br><hr>






</li>
</ul>
<h3 id="7-18-刷题-Pytorch中LSTM学习-词性标注"><a href="#7-18-刷题-Pytorch中LSTM学习-词性标注" class="headerlink" title="7.18　刷题 + Pytorch中LSTM学习 + 词性标注"></a>7.18　刷题 + Pytorch中LSTM学习 + 词性标注</h3><p><b id="log">【行】</b></p>
<ul>
<li>刷题；</li>
<li>Pytorch中LSTM学习</li>
<li>完成一半用LSTM对词性进行标注；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>在实践中才能发现更多的问题，比如今天在使用Pytorch中的LSTM建立词性标注模型的时候，就发现了很多问题，比如训练性能如何提高、超参数如何调整、模型存储及加载等等，在今天的实践中也学到了很多东西，比如各种函数的使用，如何检查代码的运行是否正常等等；</li>
<li>自己写过的代码应该系统化地保存，不要随便删掉；</li>
<li>如何使大脑更灵活、更灵动，加强运动还是多看些其他东西，或者其他什么方式；</li>
<li>利用jupyter notebook写代码不好的地方在于没有定义成类或者函数，不好调用；</li>
<li>Ruder.io这个网站应该经常去；</li>
<li>任务完成度：0.6；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>刷题；</li>
<li>完成用深度学习进行序列标注；</li>
<li>总结分词系统；<br><br><br><br><hr>





</li>
</ul>
<h3 id="7-19-刷题-Pytorch中LSTM学习-词性标注"><a href="#7-19-刷题-Pytorch中LSTM学习-词性标注" class="headerlink" title="7.19　刷题 + Pytorch中LSTM学习 + 词性标注"></a>7.19　刷题 + Pytorch中LSTM学习 + 词性标注</h3><p><b id="log">【行】</b></p>
<ul>
<li><b id="em">开始战胜自己</b></li>
<li>刷题；</li>
<li>Pytorch中LSTM学习</li>
<li>完成LSTM对词性进行标注；</li>
<li>锻炼身体</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>在使用某些库的时候，如果想更清楚一些原理，最好阅读官方文档和源码；</li>
<li>阅读好的代码能提高自己的编程水平；</li>
<li>理清思路再编码，否则可能陷入各种细节修改的局部最优解而无法获得全局最优解；</li>
<li>每天的日志必须当天写完，这也能帮助自己克服拖延的习惯；</li>
<li>锻炼身体的关键在于：<ul>
<li>要感受到锻炼位置的肌肉在发力；</li>
<li>动作要标准，速度不是关键；</li>
<li>养成良好的饮食习惯；</li>
</ul>
</li>
<li>规划的刷题内容应该更加具体；</li>
<li>任务完成度：0.6；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>二分查找、快速排序、归并排序、图复习；</li>
<li>使用pytorch建立以LSTM+CRF的命名实体系统；；</li>
<li>总结词法系统；<br><br><br><br><hr>







</li>
</ul>
<h3 id="7-20-刷题-Pytorch中LSTM学习-深度学习复习"><a href="#7-20-刷题-Pytorch中LSTM学习-深度学习复习" class="headerlink" title="7.20　刷题 + Pytorch中LSTM学习 + 深度学习复习"></a>7.20　刷题 + Pytorch中LSTM学习 + 深度学习复习</h3><p><b id="log">【行】</b></p>
<ul>
<li><b id="em">又一次战胜了自己</b></li>
<li>刷题；</li>
<li>Pytorch中LSTM学习；</li>
<li>深度学习内容复习；</li>
<li>锻炼身体；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>读书与游戏的不同<ul>
<li>分层奖励：游戏里的奖励能让人开心，欲罢不能，分层奖励让不同的人，都能获得相应的奖励，让玩家有动力持续玩下去。读书获得奖励的门槛高，不容易获得知识带来的快乐、等级提升的体验感。</li>
<li>虚拟化身的成长：游戏里角色的升级、英雄的成长、段位的提升，会让人有代入感，在乎输赢。读书不会看到肉眼的成长，语言晦涩，难有代入感。</li>
<li>角色互动：游戏提供了一个很好的游戏互动平台、社交平台，全方位提供游戏乐趣。读书是一件孤单的事情，没有互动性，以读书做社交很难。</li>
</ul>
</li>
<li>改进学习方法——将学习游戏化<ul>
<li>分层奖励——奖惩机制：设置简单有效的奖惩机制以及针对自己实际情况设置合理的奖励，要得是自己想要的、要有仪式感、要分级，要安排人员监督。</li>
<li>虚拟化身——学以致用：看完书或者学习技能后，去网上社区、论坛，帮助别人解决问题，获得成就感。</li>
<li>参与感——学习伙伴：要让伙伴对自己知识的输出做出反应，创造出互动的感觉。</li>
</ul>
</li>
<li>任务完成度：0.6；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>归并排序、图复习、刷递归类题；</li>
<li>使用pytorch建立以LSTM+CRF的命名实体系统；；</li>
<li>总结词法系统；<br><br><br><br><hr>







</li>
</ul>
<h3 id="7-21-《Python3学习笔记》-LSTM词性标记总结"><a href="#7-21-《Python3学习笔记》-LSTM词性标记总结" class="headerlink" title="7.21　《Python3学习笔记》 + LSTM词性标记总结"></a>7.21　《Python3学习笔记》 + LSTM词性标记总结</h3><p><b id="log">【行】</b></p>
<ul>
<li>《Python3学习笔记》阅读；</li>
<li>LSTM词性标记项目总结；</li>
<li>锻炼身体</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>状态有点松懈，单位时间的产出比较少，专注度有所降低；</li>
<li>对于以前做的总结没有在回顾，总结应该至少一周回顾一次，并最后汇总在专门的“总结”网页中；</li>
<li>如何快速追赶？<ul>
<li>多花时间 * 提高单位时间产出；</li>
<li>更新方法；</li>
<li>选对方向；</li>
</ul>
</li>
<li>实践中或者学习中的草稿能够看清自己走的路线，所以在学习或者实践没有进行最后总结之前，不能乱扔，此外，草稿中还有自己的书写习惯，格式习惯等等信息，这些信息对于提高思考力、纠正思考方式也许有用；</li>
<li>将公式、模型化为直观的、形象的图形，能够大大帮助自己理解，这就像费曼学习法一样，化为通俗易懂的事物（这里就是图）更能增加易懂性；<br><img src="/DIY/picture/【日志】08.png" alt="link"></li>
<li>在刷题时找规律的方法往往是试运行几步，看看结果，最好将运行过程画出来，更为直观，更易寻找规律；<center><div style="width: 20%"><img src="/DIY/picture/【日志】06.png" alt="link"></div></center></li>
<li>刷题过程中如果出现循环嵌套，一定要注意内部循环时的数值更新是否已经超出了外循环的条件；</li>
<li>刷题过程中端处理一定要单独拿出来考虑；</li>
<li>将学习过程中产生的问题（暂时没有时间去解决的）或者说需要学习的内容记录下来，一定要醒目；</li>
<li>“以点带面”式的学习方法（或者说是“辐射式”学习法），如何能带动面，前提是把项目的每个细节都研究得比较透彻，比较彻底，加以扩展实验，这才能带动面，下图这个例子不是很全面，但是具有一定的方向性；<br><img src="/DIY/picture/【日志】05.png" alt="link"></li>
<li>LSTM项目总结：<ul>
<li>结合了以前学习的吗，以前学习的起到了什么作用？<ul>
<li>用到了很多以前在“理论”上学习过的东西，以往自己陷入了学习停止不前，始终在已经学过的内容里面重复的学习，根本原因在于自己仅仅停留在了理论阶段，没有进行实践，对于学习的内容理解不深，许多在学习过程中遇到的细节很可能并没有正确理解，导致理解的扭曲，感觉到别扭，甚至说错误理解之后根本没有察觉，感到不对，过一段时间之后，就又忘了，然后又重新复习一遍，并没有在实践中纠正这些错误的理解，并且加深对正确理解角度的记忆，模型的性能、适用性也没有得到验证，回想自己学习过的机器学习课程、深度学习课程均是如此；</li>
</ul>
</li>
<li>从项目出发去学习固然是好的，但是应该建立在对该领域有一定的全局观的基础上，所以，在学习的时候，基础性的知识了解、概览性的了解是应该有的，不然完全根据项目区学习，知识点有点过于零碎、凌乱了，不好整合在一起，如果有一个全局性的掌握，就相当于有一定的指导性去学习，然后再补充填充各种细节，这样我觉得效果可能会更好，<b id="line">当然，从实用的角度和最终目的的角度出发，只要能完成目标是最好，但是从更长远的角度出发，应该具备全局观、体系性和完整性；</b></li>
<li>这些细节问题应该是现在才考虑到的吗，还是在学机器学习的时候就应该考虑到？<ul>
<li>在以前学习机器学习和深度学习理论的时候就应该掌握和理解，当然，现在也是一个重新理解的机会；</li>
</ul>
</li>
<li>深度学习框架中提供的损失函数远远多于自己在理论中学习的，这是工业与学界的区别，这能为自己提供哪些信息？</li>
<li>在理解RNN的反向传递中，参数的梯度计算一直是难以理解的，即便是在查了一些资料之后知道了有时间上的误差传递和空间上的误差传递之后也不能完全理解，其实在于对损失函数的理解，模型的损失应该是总体损失最小化，而不在于每个单体的损失最小化，也就是说在于每句话、或者说每个batch的最小化，而不在于每个词的损失最小化，这样就能够将计算所有使用相同参数矩阵的梯度求出，而不是单个考虑单词，然后就不知道如何更新梯度了；</li>
<li>如何修改、设计模型？</li>
<li>OOV词如何处理？</li>
<li>超参数如何调整？<ul>
<li>根据吴恩达的机器学习策略进行调整；</li>
<li>单独上网查询调整策略和方法；</li>
</ul>
</li>
<li>雅克比矩阵是哪两个矩阵相乘仍然不清楚，其实是刚看学习过的，但是没有仔细去研究，这也体现了自己的一种随意的态度；<ul>
<li>正确的做法应该是，学一个就掌握一个，努力做到不再重新学习一遍,实践是最好的掌握方法；</li>
</ul>
</li>
</ul>
</li>
<li>为何项目进度控制不准确：<ul>
<li>压力感不强；</li>
<li>基础不牢；</li>
</ul>
</li>
<li>不要陷入各种项目中，要时刻提醒自己最终的目标；</li>
<li>上午不该做什么，下午该做什么，明确的目标会指导行为选择；</li>
<li>从NLTK软件的角度学习NLP；</li>
<li><b id="em">使用-理解-修改</b>是学习数学和模型的几个阶段，其中，前面两个的顺序根据所学内容而定；</li>
<li>买回来的书应该都大致看下目录的内容，在学习某一资料的时候可以互为补充，不然买回来了不知道有哪些东西还不如不买；</li>
<li>在批量修改文件名的时候突然想起字符串操作应该直接能使用python中的函数进行直接操作而不是自己再写函数，这反映了基础数据类型都没掌握好，基础薄弱啊；</li>
<li>感觉什么都没学好的原因在于基础不扎实，浮躁；</li>
<li>任务完成度：0.6；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>完成LSTM+CRF的NER；</li>
<li>总结词法分析；</li>
<li>刷题，具体包括复习图模型，然后刷关于递归的题型；</li>
<li>锻炼身体；<br><br><br><br><hr>







</li>
</ul>
<h3 id="7-22-《Python3学习笔记》-LSTM词性标记总结"><a href="#7-22-《Python3学习笔记》-LSTM词性标记总结" class="headerlink" title="7.22　《Python3学习笔记》 + LSTM词性标记总结"></a>7.22　《Python3学习笔记》 + LSTM词性标记总结</h3><p><b id="log">【行】</b></p>
<ul>
<li>《Python3学习笔记》阅读；</li>
<li>刷题</li>
<li>LSTM+CRF命名实体理论学习；</li>
<li>锻炼身体</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>十一点之前必须睡觉，不然第二天精神不好；</li>
<li>记录时间行程；</li>
<li>做事专注，不要做些其他杂七杂八的小事，比如喝水，剪指甲，收拾桌面；</li>
<li>自主思考性；</li>
<li>学习复杂模型的时候，每次都以为懂了大部分之后就没有在仔细专研其他部分了，特别是解码、优化实现等类似问题之后</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>完成LSTM+CRF的NER；</li>
<li>总结词法分析；</li>
<li>刷题，具体包括复习图模型，然后刷关于递归的题型；</li>
<li>锻炼身体；<br><br><br><br><hr>







</li>
</ul>
<h3 id="7-23-刷题-CRF、HMM、Maxent"><a href="#7-23-刷题-CRF、HMM、Maxent" class="headerlink" title="7.23　刷题 + CRF、HMM、Maxent"></a>7.23　刷题 + CRF、HMM、Maxent</h3><p><b id="log">【行】</b></p>
<ul>
<li>刷题</li>
<li>CRF命名实体理论学习；</li>
<li>锻炼身体</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li><b id="em">又一次战胜了自己</b></li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>完成LSTM+CRF的NER；</li>
<li>总结词法分析；</li>
<li>刷题，具体包括复习图模型，然后刷关于递归的题型；</li>
<li>锻炼身体；<br><br><br><br><hr>






</li>
</ul>
<h3 id="7-24-刷题-最大熵模型数学公式推导-IIS算法"><a href="#7-24-刷题-最大熵模型数学公式推导-IIS算法" class="headerlink" title="7.24　刷题 + 最大熵模型数学公式推导 + IIS算法"></a>7.24　刷题 + 最大熵模型数学公式推导 + IIS算法</h3><p><b id="log">【行】</b></p>
<ul>
<li>刷题</li>
<li>最大熵模型数学公式推导 + IIS算法</li>
<li>锻炼身体</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li><b id="em">又一次战胜了自己</b></li>
<li>时间安排中如果安排上午刷2小时题，下写项目，感觉安排合理，但是如果换一下，上午写项目，下午选2小时来刷题，感觉就不好，这是为什么呢？</li>
<li>学习的时候有时候会遇到些问题或者不理解，然后就到处去查资料，其实，很多时候都可以通过自己的思考自己解决的，比如在学习NLTK最大熵分类器的时候，模型训练完了，理解模型的各种设置时，遇到有一个set_weights的函数，从字面上理解，就是设置自己的参数向量，但随后自己又在想，这不就可以添加自己的特征并赋予它定制化的权值吗，也就是加入先验知识，但是仔细一想如果要自己添加特征函数，向量中倒是可以多增加一个，但是其对应的特征名称无法添加，因为没有给出添加模板，然后自己就到处查资料如何添加，结果都没有讲，其实，回头仔细一想，其实这个添加特征的操作根本就不对，特征应该是从特征模板里生成的，这样每个特征自然可以对应于一个权值，通过训练得到，即使要添加特征，也应该是在模板里修改，在训练数据中做好相应标记，然后训练得到，而不是自己随意添加值；</li>
<li>昨天pytorch安装提示版本不对，别人有提示语，但是自己根本没有仔细琢磨，只看了python是3.7.4没注意是32位的，结果浪费了很多时间解决这个问题，我觉得这也是自己欠缺独立思考问题能力的体现；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>学习使用CRF做NER的代码，学习模块化编程，学习好的开源项目的架构；</li>
<li>刷题，具体包括复习图模型，然后刷关于与或非数的题；</li>
<li>锻炼身体；<br><br><br><br><hr>







</li>
</ul>
<h3 id="7-25-刷题"><a href="#7-25-刷题" class="headerlink" title="7.25　刷题"></a>7.25　刷题</h3><p><b id="log">【行】</b></p>
<ul>
<li>刷题</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li><b id="em">又一次战胜了自己</b></li>
<li>IDE有它的好处，使用快捷工具节省时间，比如阅读代码的时候有各种小工具可以快速定位、查看和返回；</li>
<li>自己索要用到的功能，别人早就可能想到了，甚至实现了，特别是在IDE里面，所以只需要上网找它的这个功能在哪就行了；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>学习使用CRF做NER的代码，学习模块化编程，学习好的开源项目的架构；</li>
<li>刷题，具体包括复习图模型，然后刷关于与或非数的题；</li>
<li>锻炼身体；<br><br><br><br><hr>






</li>
</ul>
<h3 id="7-26-EM、HMM、MaxEnt、MEMM、CRF-通览"><a href="#7-26-EM、HMM、MaxEnt、MEMM、CRF-通览" class="headerlink" title="7.26　EM、HMM、MaxEnt、MEMM、CRF 通览"></a>7.26　EM、HMM、MaxEnt、MEMM、CRF 通览</h3><p><b id="log">【行】</b></p>
<ul>
<li>EM、HMM、MaxEnt、MEMM、CRF 通览</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li><b id="em">又一次战胜了自己</b></li>
<li>在理解HMM、MaxEnt、MEMM、CRF几个模型时，学习相当的慢，总结有如下原因：<ul>
<li>数学基础不扎实，特别是概率与统计方面的数学知识，对于各种分布的理解不透彻直接导致了学习这些模型的时候相当的慢，理解了这部分知识点，那部分知识点又不理解，即使各部分都了解了也不会整合到一起，这就体现了知识的不系统化，不能体现知识之间的关联；比如理解马尔科夫随机场，这个知识点出现在的是图论中，自己也没有学习过这方面的知识，在看些相关资料时也是丈二和尚；</li>
<li>在学习过程中查了很多的网络博文，其实，写这些博文的人水平参差不齐，很多人根本没有理解模型的数学意思，都只是按着自己的理解来写，结果导致自己在学习的时候出现了很多矛盾的地方不能解决；- 没有对这些模型进行实现，很多细节以为自己理解了，其实还是有一定的偏差；<br><b id="em">解决方法：</b></li>
<li>（1）打牢数学基础；</li>
<li>（2）尽量查看原始论文，即提出这个模型的原始出处；</li>
<li>（3）尽量自己实现这个模型，才知道很多细节方面的理解；</li>
</ul>
</li>
<li>以最自然、最直觉的方式理解事物，打牢基础；</li>
<li>提高兴趣度，想象能用在学的知识点做些什么，因为它确实能实实在在地影响行为和专注度；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>总结词级别存在的各种主要问题和应对方法；</li>
<li>刷题<br><br><br><br><hr>






</li>
</ul>
<h3 id="7-27-句法级复习"><a href="#7-27-句法级复习" class="headerlink" title="7.27　句法级复习"></a>7.27　句法级复习</h3><p><b id="log">【行】</b></p>
<ul>
<li>句法级复习，cky</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li><b id="em">又一次战胜了自己</b></li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>总结词级别存在的各种主要问题和应对方法；</li>
<li>句法级复习；</li>
<li>刷题<br><br><br><br><hr>






</li>
</ul>
<h3 id="7-28-休息"><a href="#7-28-休息" class="headerlink" title="7.28　休息"></a>7.28　休息</h3><p><b id="log">【悟】</b></p>
<ul>
<li>造成学习断断续续的原因：<ul>
<li>领域涉及知识点很多，而自身基础不牢固；</li>
<li>长期处在舒适区造成习惯性的逃避，一遇到难的模型，在学习时间过长的情况下，就会容易松懈，放弃，选择逃避干些其他无关紧要的事，觉得自己该休息一下，事实上只是并不是自己累了，只是习惯性地想待在舒适区，过几天或者一段时间过后，遇到些其他易学、感兴趣的或者觉得更要紧的知识时，又开始学习那部分东西，把原有计划的学习体系打乱，结果学习新知识点时又没有去实现，或者重复上面的过程，导致自己根本没学到什么东西，觉得要学的东西依旧那么多，索性又开始绝望，开始用其他事情吸引自己的注意力，导致大量时间的浪费，以后基本就是重复上面的状态；</li>
<li>工作尚未找到，心理比较慌；</li>
</ul>
</li>
<li>有时候学知识不单单是学模型之类的，更重要的是思想，比如今天看最大熵模型的IIS算法，它就体现了一种逼近的思想；</li>
<li>手机里准备视频零碎时间看；</li>
<li>厉害的人可能就是把各种信息、技巧烂熟于心，在要用的时候能够直接蹦出来，而一般人也需要经历练习的过程才能把这些信息存起来，只有经过反复的练习将其内化才能在用的时候蹦出来；</li>
<li>根据阅读的目的要会选择阅读的内容，自己的行为也是同样道理；</li>
<li>现有东西不是万能的，要有能写自己定制化功能的能力，包括程序库、模型等等；</li>
<li>在学习的时候一定要知道正在学习的这个知识点在实际生活中是用于干什么的，没有目的地学习更会让人产生疲倦感，甚至反感；</li>
<li>像<a href="https://www.leiphone.com/news/201812/S0gncZu2sLSnHCZK.html" target="_blank" rel="noopener">从语言学角度看词嵌入模型</a>这类好论文从哪可以找到；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>总结词级别存在的各种主要问题和应对方法；</li>
<li>句法级复习；</li>
<li>刷题<br><br><br><br><hr>







</li>
</ul>
<h3 id="7-29-哈希结构-N元语法-句法"><a href="#7-29-哈希结构-N元语法-句法" class="headerlink" title="7.29　哈希结构 + N元语法 + 句法"></a>7.29　哈希结构 + N元语法 + 句法</h3><p><b id="log">【行】</b></p>
<ul>
<li>复习数据结构哈希结构；</li>
<li>N元语法中平滑弄清楚；</li>
<li>PCFG句法复习完；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>不要养成学习任何东西都从头看的习惯，如果自己对前面的知识点已经掌握了，就直接看自己需要的那部分，否则会浪费大量时间；</li>
<li>专注度不够；</li>
<li>pytorch官网有很多小项目可以学习；</li>
<li>做事干脆点；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>完成LSTM+CRF命名实体；</li>
<li>总结词级别存在的各种主要问题和应对方法；</li>
<li>句法级复习；<br><br><br><br><hr>






</li>
</ul>
<h3 id="7-30-Pytorch官网中LSTM-CRF作NER"><a href="#7-30-Pytorch官网中LSTM-CRF作NER" class="headerlink" title="7.30　Pytorch官网中LSTM+CRF作NER"></a>7.30　Pytorch官网中LSTM+CRF作NER</h3><p><b id="log">【行】</b></p>
<ul>
<li>理解Pytorch官网中LSTM+CRF作NER</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>今天只用在了上述收入中，并且尚未完全理解，时间利用率有点低；</li>
<li>Micheal Collin 的课是很好的，特别是他的slide；</li>
<li>时刻记住每个阶段的重点；</li>
<li>如何处理一下子理解不了的问题；</li>
<li>理解的方式、习惯、思路如何改进；</li>
<li>记录每天的时间分配情况；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>完成LSTM+CRF命名实体；</li>
<li>总结词级别存在的各种主要问题和应对方法；</li>
<li>句法级复习；</li>
<li>刷题；<br><br><br><br><hr>






</li>
</ul>
<h3 id="7-31-依存句法-感知机扩展-HMM估计参数复习"><a href="#7-31-依存句法-感知机扩展-HMM估计参数复习" class="headerlink" title="7.31　依存句法 + 感知机扩展 + HMM估计参数复习"></a>7.31　依存句法 + 感知机扩展 + HMM估计参数复习</h3><p><b id="log">【行】</b></p>
<ul>
<li>HMM估计参数复习；</li>
<li>依存句法；</li>
<li>平均感知机；</li>
<li>结构化感知机；</li>
<li>多分类感知机；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>总结词级别存在的各种主要问题和应对方法；</li>
<li>依存句法学习；</li>
<li>刷题；<br><br><br><br><hr>





</li>
</ul>
<h2 id="2019年8月"><a href="#2019年8月" class="headerlink" title="2019年8月"></a>2019年8月</h2><h3 id="8-01-基于结构化平均感知机的标注器"><a href="#8-01-基于结构化平均感知机的标注器" class="headerlink" title="8.01　基于结构化平均感知机的标注器"></a>8.01　基于结构化平均感知机的标注器</h3><p><b id="log">【行】</b></p>
<ul>
<li>研究基于《A Good Part-of-Speech Tagger in about 200 Lines of Python》论文感知机标注器源代码；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>要思考如何解决单本书或者单个视频教程带来的片面性理解；</li>
<li>使模糊的模型、数学关系变得清晰，这是理解一个模型的标志；</li>
<li>博客可以作为别人评判你水平的标志之一；</li>
</ul>
<p><br><br><br></p>
<hr>






<h3 id="8-08-28岁，启程"><a href="#8-08-28岁，启程" class="headerlink" title="8.08　28岁，启程 . . ."></a>8.08　28岁，启程 . . .</h3><h4 id="0-Tips"><a href="#0-Tips" class="headerlink" title="0. Tips"></a>0. Tips</h4><ul>
<li><div><div class="fold_hider"><div class="close hider_title">面试流程</div></div><div class="fold">
<center><div id="fold"><img src="/DIY/picture/【待做】面试的流程.png"></div> </center>
</div></div></li>
<li>以项目带动基础知识复习和学习，传统方法和深度学习方法交替使用；</li>
<li>工作相关关键字：NLP研究员、NLP算法工程师、NLP工程师、自然语言处理、机器学习；</li>
<li>提高有效工作时间；</li>
<li>写博客、参加比赛，积累证明自己能力的途径；</li>
<li>考虑自己与其他求职者最大的不同；</li>
<li>展现活力、积极与真诚；</li>
<li>简历上不要写自己不太懂的东西；</li>
</ul>
<h4 id="1-编程类"><a href="#1-编程类" class="headerlink" title="1. 编程类"></a>1. 编程类</h4><p><b id="em">数据结构与算法：</b><br><b id="line">一方面要自己实现算法，另一方面要积累各种各样的解题思路</b></p>
<ul>
<li>第一梯队：复杂度分析、链表、二叉树、二分查找、快速排序、归并排序、回溯法、动态规划、分治、贪心、枚举</li>
<li>第二梯队：正则表达式、字符串、栈、队列、堆、数组、图 {DFS遍历、BFS遍历、最大（小）生成树算法（Kruskal、Prim）、最短路径（Dijkstra、Floyd）}</li>
<li>第三梯队：哈希、集合；</li>
</ul>
<p><b id="em">python语言：</b></p>
<ul>
<li>python语言自身的优缺点需要掌握；</li>
<li>各种常用类库的掌握和运用；</li>
<li>熟悉bug调试的技巧；</li>
<li>熟悉PyCharm的使用技巧；</li>
</ul>
<p><b id="em">Linux操作系统：</b></p>
<ul>
<li>会使用简单的命令；</li>
<li>使用python进行编程；</li>
</ul>
<p><b id="em">数据库：</b></p>
<ul>
<li>数据库操作语句；</li>
</ul>
<p><b id="em">计算机网络知识：</b></p>
<ul>
<li>TCP三次握手；</li>
</ul>
<h4 id="2-自然语言处理"><a href="#2-自然语言处理" class="headerlink" title="2. 自然语言处理"></a>2. 自然语言处理</h4><p><b id="em">要求：</b></p>
<ul>
<li><b>掌握</b> 从头到尾的各个环节（包括特征工程（文本预处理））以及各个环节主要的解决方法，各自的优缺点有哪些；</li>
<li>从各环节的实现开始建立自己的NLP系统（提高整合能力）；</li>
<li>深入了解模型所用的算法及其原理和推导，算法本身的优劣性，使用的条件限制是什么；</li>
<li>练习使用NLTK、Spacy、Gensim、Pytorch、FastText等相关软件</li>
</ul>
<p><b id="em">内容：</b></p>
<ul>
<li>传统模型：N元模型、HMM、MaxEnt、CRF、EM、MMseg、PCFG、PLSA、LDA</li>
<li>词级别：分词（基于离散特征的CRF、BILSTM-CRF）、词性标注、命名实体识别</li>
<li>句级别：浅层句法分析、依存分析、词义级分析（重点掌握内容）、句义级分析</li>
<li>篇级别：文本分类、情感分类</li>
</ul>
<p><b id="em">规划的项目：</b></p>
<ul>
<li>项目列表：<code>分词、词性标记、实体识别、依存句法分析、短语句法分析、文本分类、文本聚类、词义消歧、句义消歧、情感分析、相似度计算、意图识别、文本摘要</code>；</li>
<li><p>选择标准：</p>
<ul>
<li>尽量包含常用内容；</li>
<li>包含内容较为全面最好；</li>
</ul>
</li>
<li><p>候选项目</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/51279338" target="_blank" rel="noopener">常见30种NLP任务的练手项目</a></li>
<li><a href="https://github.com/FudanNLP/nlp-beginner" target="_blank" rel="noopener">邱锡鹏建议的几个小项目</a></li>
<li><a href="http://www.aboutyun.com/thread-24696-1-1.html" target="_blank" rel="noopener">自然语言处理从上手到进阶【项目资源库汇总】</a></li>
<li><a href="https://www.pytorchtutorial.com/github-nlp-with-pytorch/" target="_blank" rel="noopener">Github 上 Star 过千的 NLP 相关项目</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2019-03-04-8" target="_blank" rel="noopener">一些NLP项目资源</a></li>
<li><a href="http://www.sohu.com/a/304619940_473283" target="_blank" rel="noopener">NLP领域最优秀的8个预训练模型（附开源地址）</a></li>
</ul>
</li>
<li><p>规划项目：</p>
<ul>
<li>序列标注：分词系统、词性标注、命名实体识别系统 <ul>
<li>(bi)LSTM+CRF序列标记项目（设计gensim使用产生词向量，LSTM可以更换为RNN和GRU，CRF训练需要用到HMM中的训练方法，CRF原理与最大熵模型极其相似，使用Pytorch深度学习框架）（参考【ipynb】Deep Learning for Natural Language Processing with Pytorch）</li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649447728&amp;idx=1&amp;sn=af6b37d4f7ef9826e768b7fa999e33b1&amp;chksm=82c0b4b4b5b73da2656501ec5ce9d36d1ab213425ca18451e4cf3f8a95aa1f6b387265a0248a&amp;scene=0&amp;xtrack=1&amp;key=ae82afc765e556e1f31e0f8b17c324af38deef2357ee690857c93b62dd5faf8b4582b8984d81ac732af9c517598052612abe668dc36604231e5b4d3cb72ec0278b580ad391c13b654c0879411a009c71&amp;ascene=14&amp;uin=MTkzNzg2MTUxNA%3D%3D&amp;devicetype=Windows+10&amp;version=6206081a&amp;lang=zh_CN&amp;pass_ticket=6JRPZMz8qFg7mHiKmKV6JAd63j5AUTZnVz2h9e1nHlwMICQQZiGqnOC2%2BPxErDiI" target="_blank" rel="noopener">最新中文分词改进版网址</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247497183&amp;idx=1&amp;sn=49ce9df2cf3a33796b64b6dbe268b714&amp;chksm=96ea2a5fa19da34969c85c9100fdcaa8b5e3634b97042266e526e36cf602cf6240faf560c780&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;key=ae6bf02451889696cca8174000da7b33a4061faeb91656bbc5eacdbc62980ee296f7421547f41e565de7c52f2f2f6254e171336d21f7876659590e736614a65a8741416edc11bf7ce80f29ba49a84b2f&amp;ascene=1&amp;uin=MTkzNzg2MTUxNA%3D%3D&amp;devicetype=Windows+10&amp;version=6206081a&amp;lang=zh_CN&amp;pass_ticket=LdxMy%2FDqJPB0yCP7P2nfJ0DCRCjDFln2Gnjv7zNZXBXNN8nMy50qB26WPDRiBioc" target="_blank" rel="noopener">复旦大学邱锡鹏教授：词法、句法分析研究进展综述</a></li>
<li>MaxEnt字标注（复习+NLTK的使用）</li>
</ul>
</li>
<li>句法分析系统（主要是依存句法分析）<ul>
<li>基于图的依存句法分析</li>
<li>基于转移的依存句法分析</li>
</ul>
</li>
<li>语义分析系统<ul>
<li>指代消解问题</li>
</ul>
</li>
<li>文本聚类与分类<ul>
<li>文本特征的选择方法（涉及TF-IDF、信息增益、卡方统计、互信息）</li>
<li>分类器设计（涉及朴素贝叶斯、SVM、KNN、决策树、逻辑回归等机器学习方法，并且熟悉使用sklearn库）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="3-机器学习"><a href="#3-机器学习" class="headerlink" title="3. 机器学习"></a>3. 机器学习</h4><p><b id="em">模型:</b><br>SVM算法（最为常用和权威的分类方法）、感知机（神经网络的基础组件部分）、朴素贝叶斯、决策树（涉及信息学方面的内容）、逻辑回归（比较简单和常用）、K近邻算法K-means、提升算法（集成算法的一种）</p>
<p><b id="em">策略:</b><br>参考吴恩达的深度学习系列课程中关于结构化机器学习项目和其其新书《Machine Learning Yearing》，参考其中在实践方面的经验和策略；</p>
<ul>
<li>过拟合（欠拟合）以及如何解决</li>
<li>偏差与方差（机器学习系统的设计和建议）</li>
<li>正则化</li>
<li>查准率与查全率</li>
<li>特征稀疏问题解决方法</li>
<li>各种损失函数及其适用场景</li>
</ul>
<p><b id="em">要求:</b><br>Scikit-learn、Numpy、Pandas、Matplotlib、Seaborn的使用</p>
<p><b id="em">评估:</b><br>指标包括：F1、Precision、Recall</p>
<h4 id="4-深度学习"><a href="#4-深度学习" class="headerlink" title="4. 深度学习"></a>4. 深度学习</h4><p><b id="em">内容：</b></p>
<ul>
<li>Word-Embedding、RNN、LSTM、GRU、Attention、Seq2Seq、Transformer、encode-decoder、递归神经网络；</li>
<li>在吴恩达的课程中，涉及到了机器学习项目策略的问题和各种评判指标，这对于实践来说非常重要，也是需要仔细学习的内容；</li>
</ul>
<p><b id="em">要求：</b></p>
<ul>
<li>熟练使用Pytorch软件；</li>
</ul>
<h4 id="5-应聘"><a href="#5-应聘" class="headerlink" title="5. 应聘"></a>5. 应聘</h4><ul>
<li><a href="http://www.pkudodo.com/2019/03/10/1-9/" target="_blank" rel="noopener">面试体会|微软、头条、滴滴、爱奇艺NLP面试感想</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36096340" target="_blank" rel="noopener">BAT三公司NLP实习offer共勉</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/45802662" target="_blank" rel="noopener">别求面经了！小夕手把手教你斩下NLP算法岗offer原文链接</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/36387348" target="_blank" rel="noopener">暑期实习NLP算法岗面经总结</a><br><br><br><br><hr>







</li>
</ul>
<h3 id="8-09-动态规划-文本分类"><a href="#8-09-动态规划-文本分类" class="headerlink" title="8.09　动态规划 + 文本分类"></a>8.09　动态规划 + 文本分类</h3><p><b id="log">【行】</b></p>
<ul>
<li>刷题动态规划</li>
<li>文本分类项目开头</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>别人研究了很久设计出来的算法，你凭什么能用很短时间就以为理解了，会用不能说明是理解了原理和思想，所以在学习某些前人的经典算法时，需要静下心来体会设计者是如何一步步思考的，其中的思想精髓是什么；</li>
<li>建立自己的各种沟通方式，例如<a href="https://www.iamlightsmile.com/" target="_blank" rel="noopener">LightSmile’blog</a>并且将所学的知识点实现，集成系统化，一方面，检验自己所学知识是否扎实，提高实现能力，系统化集成能力，另一方面广泛的沟通和交流；</li>
<li>领域技术各式各样，如何挑选性价比高的技术，理论进行学习，这个值得思考；</li>
<li>很多模型已经有实现了，所以不用自己再造轮子，但是，别人的轮子是否好用，是否有缺陷，需要加以考量。并不是说有了轮子自己就不用造了，在前期的学习过程中，自己学着写轮子还是很有必要的，它有助于我们理解知识点和知道有哪些缺陷；</li>
<li>这个项目应该达到的目的有哪些：<ul>
<li>练习使用各种传统分类器，以及如何将传统二分类器转换为多分类器；</li>
<li>学习文本分类的各个流程和技术；</li>
<li>学习将深度技术应用于文本分类中；</li>
</ul>
</li>
<li>在利用TF-IDF进行特征选择的时候，维基百科上面很清楚的写了一部分关于它的理论和缺陷；</li>
<li>真正喜欢一样东西是什么感觉，是什么状态，网络搜索一下:<ul>
<li>像打游戏一样，但怎么才能做到像打游戏一样呢？</li>
</ul>
</li>
<li>先学会使用轮子，和学习其基本原理，等工作了再考虑是否需要自己实现一遍；<br><br><br><br><hr>







</li>
</ul>
<h3 id="8-11-CS224N第01讲"><a href="#8-11-CS224N第01讲" class="headerlink" title="8.11　CS224N第01讲"></a>8.11　CS224N第01讲</h3><p><b id="log">【行】</b></p>
<ul>
<li>CS224N第一课</li>
<li>规划下一阶段的学习内容；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>写博客的目的在哪？应该记录哪些内容？分清笔记和博客的作用？<ul>
<li>将学习的体系性的课程、每天的体会感悟记录在博客上；</li>
<li>将零散的、补充性的知识点笔记记在笔记中；</li>
</ul>
</li>
<li>利用好自己<b id="em">独处</b>的时间学习和提高，把娱乐的时间留在跟亲朋在一起；</li>
<li>python的学习并不是最主要的内容，因为它的<b id="em">被替代性很强</b>，python中其实大部分的库都很少有机会用到，所以对于常用的要熟悉，对于不常用的，有个印象就行了，不要把大把时间花在研究python本身上面，花在更为重要的地方；</li>
<li><b id="em">下一阶段的主要学习内容、选择参考以及注意点：<ul>
<li>内容：<ul>
<li>2017年的CS224N中文版课程；</li>
</ul>
</li>
<li>参考因素：<ul>
<li>成体系化且比较初级，可以作为DeepNLP生涯的展开；</li>
<li>现在一般使用的都是基于深度学习的NLP，能跟上时代的步伐，并且能适应工作的需要；</li>
<li>有教授讲解，理解更为透彻，迅速，这是从书本学习不具有的优势；</li>
</ul>
</li>
<li>注意：<ul>
<li>传统NLP方法依然要学习，只不过时间不应该在这个阶段，因为传统学习提供了另一些理解NLP的视角，并且它也拥有自己的优势，可以与DeepNLP对比学习；</li>
<li>要慢慢改掉原来的学习习惯：（1）一定要阅读相关文章；（2）一定要自己亲自实践；</li>
<li>一步一个脚印，走扎实了，争取不再回锅；<br><br><br><br><hr>







</li>
</ul>
</li>
</ul>
</b></li>
</ul>
<h3 id="8-12-CS224N第02、03讲"><a href="#8-12-CS224N第02、03讲" class="headerlink" title="8.12　CS224N第02、03讲"></a>8.12　CS224N第02、03讲</h3><p><b id="log">【行】</b></p>
<ul>
<li>CS224N第02、03讲，词向量模型；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>在从视频资源进行学习的时候，特别是英文资源，因为自己听力不是很好，所以有时候只顾着看中文字幕而忽视了PPT里的内容；</li>
<li>喜欢代码这种整齐划一的表现形式；</li>
<li>喜欢这种利用强大工具延伸自己能力的方式；<br><br><br><br><hr>







</li>
</ul>
<h3 id="8-14-CBOW-Hierarchecal-Softmax-实现词向量"><a href="#8-14-CBOW-Hierarchecal-Softmax-实现词向量" class="headerlink" title="8.14　CBOW + Hierarchecal Softmax 实现词向量"></a>8.14　CBOW + Hierarchecal Softmax 实现词向量</h3><p><b id="log">【行】</b></p>
<ul>
<li>实现CBOW+Hierarchecal Softmax的词向量；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>对数据的观察很重要，每个语料集都有自己的特点，模型需要根据这些特点调整参数以便更好地模拟语料；</li>
<li>对比已有的库，训练同样的语料，看自己的差距在哪里；</li>
<li>要知道如何调整，要有策略，从machine learning yearing上学；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>理解词向量到底捕捉的是语言的什么信息，是怎样捕捉的；</li>
<li>完善CBOW+Hierarchecal模型，实现模块化、系统化，考虑是否需要把一个长句分成短句进行训练；</li>
<li>开始写Skip-Gram+Negative sampling模型；<br><br><br><br><hr>







</li>
</ul>
<h3 id="8-15-CBOW-Negative-Sampling-实现词向量"><a href="#8-15-CBOW-Negative-Sampling-实现词向量" class="headerlink" title="8.15　CBOW + Negative Sampling 实现词向量"></a>8.15　CBOW + Negative Sampling 实现词向量</h3><p><b id="log">【行】</b></p>
<ul>
<li>实现CBOW+Negative Sampling的词向量；</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>笔记《Word2Vec中的数学》写得很好，从预备知识到背景知识再到模型的原理，模型的实践细节，待处理问题，优缺点都有，参考资料，这个可以作为自己写博客的一个模板；</li>
<li>自己应该更加注重细节的问题，在学习《Word2vec中的数学》中基于Negative Sampling的模型（28页）一节时，明明在第一段就写了Negative Sampling是NCE（Noise Contrastive Estimation）的一个简化版本，这本可以提醒自己在理解上可以从NCE出发，结果学习的时候直接忽视了这句，到后来不能理解的时候又从网上才看到了关于NEG的根本性原理NCE，这应该是提醒自己做科研一定要细心，不能粗枝大叶；</li>
<li>再把CS224N前面学过的看一遍，对softmax loss求导进行全面推导；</li>
<li>numpy库的掌握很重要；</li>
</ul>
<p><b id="em">明日安排：</b></p>
<ul>
<li>将CBOW的两种实现方式集成；</li>
<li>写出Skip-Gram的两种实现方式；</li>
<li>集成实现CBOW、Skip-Gram的两种实现方式；<br><br><br><br><hr>







</li>
</ul>
<h3 id="8-21-词向量项目总结"><a href="#8-21-词向量项目总结" class="headerlink" title="8.21　词向量项目总结"></a>8.21　词向量项目总结</h3><p><b id="log">【行】</b><br>实现了一下相关模型：</p>
<ul>
<li>CBOW + Hierachecal Softmax；</li>
<li>CBOW + Negative Sampling；</li>
<li>Skip Gram + Hierarchecal Softmax;</li>
<li>Skip Gram + Negative Sampling;</li>
<li>Glove;</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>编程会提高对知识的理解，我认为其中的原理之一可能是编程的过程其实就是让自己慢下来，有更多的时间思考模型的原理和意义；</li>
<li>了解教学视频要求的知识背景很重要，否则会浪费很多时间反复地、零散化地去补课程所涉及到的基础知识，再一次强调了基础的重要性；</li>
<li>每个模型背后的思想才是最重要的东西；</li>
<li>观察数据很重要，任何数据都有自己的特点；</li>
<li>结果有问题时要仔细分析可能出现问题的原因，如果模型没有问题，就调整参数，如果调整参数得不到相近答案，则调整方法；</li>
<li>分析各类学习资源的优缺点，比如视频资源，虽然有些细节讲得很好，但是缺乏系统性，所以在学完视频时自己还需要从其他资料获取对这部分知识的完整理解；再比如书，虽然具有一定的系统性和细节性，但是它具有延迟性；论文资源，虽然具有时效性，但是细节不一定讲得很明白，跨语言理解的准确性有待考证；</li>
<li>顺着主要路线走，对于路线中需要的支路线，找时间补就行，没有必要为了支路线的内容而将本该用于主路线的时间花在支路线上；</li>
<li>博客应该是自己的学习笔记，可以供其他人浏览和评价；</li>
<li>对于实现某些模型的时候，用jupyter noteboo完全就可以了，但要注意的是，尽量写成模块的形式以便于自己可以快速的把这些模型转换为可以通用的模块；</li>
<li>自然语言中运用了很多的数学模型去模拟某些现实中中的情境，这都是人们基于对数学的个人理解从而运用的，很多时候都没有完全的理论依据，所以给人的想象空间很大，但是，也不是凭空想象的，是基于对数学知识、模型的理解而来的，所以数学才是根本；</li>
<li>要注意在循环中的索引变量 i ，可能与其他变量名字相同；</li>
<li>注意jupyter notebook会存储以前的变量，在编程测试的时候尤为需要注意；</li>
<li>写项目时，分清哪些是内在参数和外部调用参数；</li>
<li>在写项目的时候最好先用简单的实例验证局部模块是否完全正确；</li>
<li>模仿好的库的架构；</li>
<li>数据的预处理应该根据实际任务和数据的实际情况而调整，不应该一成不变；</li>
<li>模块中私有变量和其他变量的权限处理；</li>
<li>在写模型的时候任何想法总结先记录；</li>
<li>如何利用好公众号、知乎、paperweekly等实时资源；</li>
<li>词向量模型是别人研究了好几个月甚至好几年的模型，在研究过程中遇到了各式各样的细节性问题，在研究这些细节的时候增加了对模型的理解，而自己在学习这些词向量模型的时候，首先看的是原理，接着是短时间的实现，实现的过程中，遇到的问题远远少于研究者遇到的问题，导致了很多未见到的细节都没有得到足够的优化，最后效果的累积起来就导致了差距的产生，所以在学习的初期阶段，不用太深入地去了解，行程一定的理解并实践已经足够，这时候的目标是了解深度学习在整个nlp领域的应用，最后再回过头来深入研究自己选择的领域；<br><br><br><br><hr>







</li>
</ul>
<h3 id="8-28-建站"><a href="#8-28-建站" class="headerlink" title="8.28　建站"></a>8.28　建站</h3><p><b id="log">【悟】</b></p>
<ul>
<li>阅读博文时可能会提到另一些知识点（比如今天读到的特征为什么要离散化和怎么离散化）而自己不太清楚甚至不知道，对于这些知识点，我们可以单独拿出来研究，在知乎上搜索理解，而一些高分回答的作者，往往对这个行业有很深的理解，我们又可以去看看他们的博文从而积累更多的知识，从零散的知识点逐渐积累成系统性知识点；</li>
<li>思考自己为什么做项目为什么老是延迟预计完成时间；</li>
<li>LR是一种在线学习（动态扩展）的算法，自己以前却不知道，或者在知道感知机是在线学习算法的时候自己也没有去思考在线学习到底有哪些特点，哪些算法属于在线学习；</li>
<li>softmax公式推导的启示：<ul>
<li>对于算法公式的理解，由于自己的知识限制，可能理解得有偏差，这时候就需要积累一些纠正这些偏差理解的方法，比如：<ul>
<li>利用编程实现算法，通过结果验证理解；</li>
<li>利用高价值博文、评论验证；</li>
</ul>
</li>
<li>原来对于softmax的理解是只求目标类的梯度，也就是one-hot为1的梯度，其实这是错误理解了，因为自己把one-hot代入了损失函数然后才来求导，结果导致one-hot为0的类的梯度通通为0，这里明显是把损失函数和梯度求导搞混了，也体现了自己数学不扎实的缺点；</li>
</ul>
</li>
<li>学着从舒适区里走出来；</li>
</ul>
<p><b id="log">【待】</b></p>
<ul>
<li>jupyter lab实用的魔术命令；</li>
</ul>
<p><br><br><br></p>
<hr>









<h2 id="2019年9月"><a href="#2019年9月" class="headerlink" title="2019年9月"></a>2019年9月</h2><h3 id="9-02-计划"><a href="#9-02-计划" class="headerlink" title="9.02　计划"></a>9.02　计划</h3><p><b id="log">【悟】</b></p>
<ul>
<li>缺乏一种独立思考的能力；</li>
<li>简单计划—&gt;上路—&gt;在路上观察自己所需内容—&gt;学习内容—&gt;接着走，在自己对这个领域并没有深刻认识的情况下，不要忙着搜集大量相关内容写详细的计划，因为自己并不知道重点、难点在哪里，特别是在没有人指导的情况下，这种计划用一个经济学领域的话来说就是“计划经济”，而先简单计划，边干活边观察需要的东西，然后按需学习，这种计划更像是“市场经济”，显然，市场经济更能适应社会；</li>
<li>在B站看外国教学视频时很多英文的字幕，于是很多人在评论区抱怨没有中文字幕，我觉着这也是限制自己能力，依靠他人的一种做法，如果真的想变得厉害，我觉得应该学习英文；</li>
<li>多个逻辑回归和softmax的使用区别；<br><br><br><br><hr>






</li>
</ul>
<h3 id="9-06-Do-Nothing"><a href="#9-06-Do-Nothing" class="headerlink" title="9.06　Do Nothing"></a>9.06　Do Nothing</h3><p><b id="log">【悟】</b></p>
<ul>
<li>测试在前（即想到测试用例用于验证程序小部件正确性，用于提前发现问题），开发在后；</li>
<li>代码调试功底，如何快速地找出错误——熟练设置断点、单步跟踪、查看内存、分析调用栈；</li>
<li>思考清楚再编码，否则只会越改越乱；</li>
<li>良好的代码命名和缩进对齐习惯；</li>
<li>简历中要注意“了解”、“熟悉”和“精通”几个词的使用；<br><br><br><br><hr>





</li>
</ul>
<h3 id="9-15-计划"><a href="#9-15-计划" class="headerlink" title="9.15　计划"></a>9.15　计划</h3><p><b id="log">【悟】</b></p>
<ul>
<li>不管学习什么东西，都应该把每个细节都全部弄明白，从头到尾自己推导一遍（不是跟着书），对于有些细节不要看一眼以为自己懂了就不去深究，这就是所谓地打牢基础，避免日后遇到之后就跟没有学过一样，或者完全想不起原理，这就浪费了更多的时间，从原理上弄明白了之后最好实践一下，这样能够更清楚使用的效果、适用的范围等等，同时也是对理论地加深；<br><br><br><br><hr>





</li>
</ul>
<h3 id="9-16-学习CS224N（2017版）"><a href="#9-16-学习CS224N（2017版）" class="headerlink" title="9.16　学习CS224N（2017版）"></a>9.16　学习CS224N（2017版）</h3><p><b id="log">【行】</b></p>
<ul>
<li>学习CS224N（2017版）Lecture9和Lecture10</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>学习基于传统ML的NLP能够帮助我们了解模型的组成和各个细节，因为在基于DL的NLP中，很多原来手工设计的特征会被自动提取出来，但是很多时候提取的特征是什么意思并不十分明确，借助对传统ML中NLP手工设计特征的学习来帮助理解深度学习自动提取特征是很有帮助的，它帮助我们明白DeepNLP底层到底是在做什么；</li>
<li>每个课程的侧重点或者理解方式不一样，比如在讲解LSTM各个门的作用时，吴恩达的课程并没有很清楚的讲解其中的具体作用，可能只是为了普及模型，而CS224N课程类的LSTM则把各个门的作用讲解得比较透彻，了解课程的适用对象和特点很重要；</li>
<li>将视频课程中比较重要的语句记录在书中，不要光凭记忆，好记性不如烂笔头；</li>
<li>模型是干什么的，由什么推导而来的，有什么优势，有什么局限；<br><br><br><br><hr>





</li>
</ul>
<h3 id="9-17-学习CS224N（2017版）"><a href="#9-17-学习CS224N（2017版）" class="headerlink" title="9.17　学习CS224N（2017版）"></a>9.17　学习CS224N（2017版）</h3><p><b id="log">【行】</b></p>
<ul>
<li>学习CS224N（2017版）Lecture11、Lecture13、Lecture14</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>学习模型的时候到底该掌握什么内容；</li>
<li>对于CS224N课程，从上课的内容来看，很多时候如果没有以前的NLP积累，并不是那么容易明白，而在课堂上的提问环节，很多学生能够提出比较关键的问题，我认为这可能是他们在上课前就对相关知识进行了学习，也就是预习，所以在上课时，一方面对自己已经理解的内容进行确认，另一方面重点关注了自己没有明白的内容，这样的学习方式值得自己思考，另一个问题是，课堂上的内容往往讲得比较精简，很多细节和拓展并没有讲，但是给了很多阅读参考资料需要学生自己去学习，我觉得这也是锻炼学生自主学习、理解细节和拓展的方法，锻炼学生独立学习和继续深入下去的能力；</li>
<li>在学习CS224N时，最重要的参考是hanck的课程笔记，看了她的整个网站，结合自己学习时的问题，总结如下：<ul>
<li>基础知识（包括数学知识，机器学习知识，深度学习知识，NLP知识）决定了我们的吸收速度，同时让我们不再关心基础结构而关注模型思想、创新点等根本性重点；</li>
<li>语言问题，绝大部分的优秀的讲解视频时基于英文的，虽然也有翻译过来的，但是因为翻译者的水平参差不齐导致了翻译的准确性得不到稳定的保证，所以加紧时间学习英语也是一个迫切的问题，特别是需要阅读大量的英文论文；</li>
<li>看视频的习惯问题，对于不太明白的内容没有反复地去观看和揣摩，并且在学习的过程中没有记录笔记，在学习的过程中只是被动的被领着走，并没有主动的探索和思考，这对于知识的吸收并不好；</li>
</ul>
</li>
<li>云计算租赁提高效率；</li>
</ul>
<p><br><br><br></p>
<hr>





<h3 id="9-18-学习CS224N（2017版）"><a href="#9-18-学习CS224N（2017版）" class="headerlink" title="9.18　学习CS224N（2017版）"></a>9.18　学习CS224N（2017版）</h3><p><b id="log">【行】</b></p>
<ul>
<li>学习CS224N（2017版）Lecture15、Lecture16、Lecture17、Lecture18</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>建立自己的一套读书笔记系统，从《自然语言处理综论》、《统计方法学习》中开始设计，这两本书笔记和记号较多，记号中最好只是形式的不同而非颜色的不同，这样能最大化较少对外部工具的需求；可以参考英文书籍中旁边的段落总结符号；</li>
<li>是否考虑将英语作为主要研究语言，第一、英文是使用最为广泛的语言；第二、英文较中文而言，没有那么复杂，并且研究者也非常多；</li>
<li>在理解递归神经网络的升级版时，线性代数的矩阵特点使特征能够转换，这是数学的力量，在理解树结构和水平线性结构结合提高了树结构的运行效率使之能够并行运行，这是数据结构的力量，在指代消解过程中，利用各种语言学的知识，较少了运算量，提高了效率和准确率，这是语言学的力量，所以说，对涉及各个重要学科有扎实地了解能够帮助自己建解决各种遇到的困难，为解决他们提供了丰富的工具，也在解决问题提供了更多的思路；</li>
<li>大量阅读各种各样的论文，开阔自己的眼界，<b id="line">眼界是决定创新程度的关键因素</b>，并且，大量阅读还可以积累各种各样解决问题的办法，以及各种新颖的思考问题的角度；</li>
<li>提高工程能力，快速迭代模型，把更多时间留给思考和创新；</li>
<li>学习课程，应该做的是把课程讲的内容通过查阅更多资料了解完整细节、再通过代码实践式地<b id="em">加厚</b>，而不是简单了解、浏览式甚至跳阅地<b id="em">压薄</b>（除了带了某种专门的目的学习除外）；</li>
</ul>
<p><br><br><br></p>
<hr>




<h2 id="2019年10月"><a href="#2019年10月" class="headerlink" title="2019年10月"></a>2019年10月</h2><h3 id="10-01-学习"><a href="#10-01-学习" class="headerlink" title="10.01 学习"></a>10.01 学习</h3><p><b id="log">【行】</b></p>
<ul>
<li>刷《剑指Offer》链表两题；</li>
<li>整理英文学习资料；</li>
</ul>
<p><br><br><br></p>
<hr>




<h3 id="10-16-找工作复习实践计划"><a href="#10-16-找工作复习实践计划" class="headerlink" title="10.16　找工作复习实践计划"></a>10.16　找工作复习实践计划</h3><h4 id="1-工程"><a href="#1-工程" class="headerlink" title="1. 工程"></a>1. 工程</h4><p><b id="em">数据结构和算法</b></p>
<ul>
<li>学习面试经验；</li>
<li>内容掌握：<ul>
<li>链表、二叉树、归并排序、快速排序、二分查找；</li>
<li>回溯法、动态规划、分治、递归等算法思想；</li>
<li>栈、队列、堆、图 <font size="1px">{DFS遍历、BFS遍历、最大（小）生成树算法（Kruskal、Prim）、最短路径（Dijkstra、Floyd）}</font></li>
<li>字符串、正则表达式；</li>
<li>哈希、集合；</li>
</ul>
</li>
</ul>
<p><b id="em">编程语言</b><br><b id="em">Linux系统</b><br><b id="em">数据库与网络知识</b></p>
<h4 id="2-理论"><a href="#2-理论" class="headerlink" title="2. 理论"></a>2. 理论</h4><p><b id="em">自然语言处理</b></p>
<ul>
<li>工程上的处理：<ul>
<li>文本预处理环节（中文和英文）可以参考《Python文本分析》；</li>
<li>找工作前必完成任务（参考《自然语言处理综论》、《统计自然语言处理》、《基于深度学习的自然语言处理》、CS224N课程和Andrew Ng的深度学习课程第五门课，可以参考高阶书籍《多语自然语言处理》）：<ul>
<li>序列标注（Segmentation、POS、NER）；</li>
<li>句子分析（包括语法分析和依存分析）；</li>
<li>文本分类、聚类；</li>
<li>对话系统；</li>
</ul>
</li>
<li>找工作进行时完成任务；<ul>
<li>文本摘要；</li>
<li>语义分析；</li>
</ul>
</li>
<li>要求：<ul>
<li>清楚每个任务的难点在哪；</li>
<li>清楚每个模型的优缺点，对于缺点需要想一些改进措施；</li>
<li>清楚每个任务的传统方法和深度学习方法；</li>
<li>清楚每个任务目前最优的方案是什么；</li>
<li>清楚每个任务的评测标准；</li>
<li>清楚哪些库可以实现相应任务的功能；</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><b id="em">深度学习</b></p>
<ul>
<li>主要是对自然语言处理相关的内容进行复习；</li>
<li>熟悉框架Pytorch；</li>
<li>清楚各种激活函数的优缺点及所用之处；</li>
<li>清楚超参数调参、正则化以及优化，这部分可以参考Andrew Ng的深度学习课程；</li>
</ul>
<p><b id="em">机器学习</b></p>
<ul>
<li>主要参考《统计学习方法》<ul>
<li>原因：<ul>
<li>本书包括了最为常用的机器学习方法；</li>
<li>有详尽的数学推导，便于理解深层原理；</li>
<li>基本上学习过一遍，再过一遍主要是为了实践从而有更深的理解和记忆，并且效率与更高；</li>
</ul>
</li>
<li>要求：<ul>
<li>理解基本原理；</li>
<li>使用scikit-learn进行实践；</li>
<li>清楚各种损失函数；</li>
</ul>
</li>
<li>模型复习顺序：<ul>
<li>SVM（连带感知机） –&gt; 决策树 –&gt; k近邻 –&gt; 提升方法 –&gt; EM</li>
</ul>
</li>
</ul>
</li>
<li>机器学习的策略，评估方法</li>
<li>特征工程</li>
<li>后续内容：<ul>
<li>《统计学习方法》主要从数学推导的视角讲解的模型，更多的相关视角可以从其他机器学习相关书籍中寻找；</li>
<li>深入模型的数学原理上的假设限制，适用条件等等；</li>
</ul>
</li>
</ul>
<p><b id="em">数学</b></p>
<ul>
<li>微积分：<ul>
<li>导数、偏导数、梯度；</li>
<li>雅克比矩阵（一阶求导）和海森矩阵（二阶求导）；</li>
</ul>
</li>
<li>线性代数：<ul>
<li>矩阵乘法；</li>
<li>矩阵分解；</li>
</ul>
</li>
<li>概率与统计：</li>
</ul>
<p><br><br><br></p>
<hr>




<h2 id="2019年11月"><a href="#2019年11月" class="headerlink" title="2019年11月"></a>2019年11月</h2><h3 id="11-14-日常"><a href="#11-14-日常" class="headerlink" title="11.14　日常"></a>11.14　日常</h3><p><b id="log">【悟】</b></p>
<ul>
<li>对于已买的书，即使没有时间将内容全部或者部分看完，至少看完<b>目录</b>和<b>前言</b>，了解书里大致讲的内容，同时也扩展一下自己的概念库，知道概念对应于哪些内容，要用到的时候有个印象，也不至于要用的这些知识点的时候一点也不知道，书也白买了；</li>
</ul>
<p><br><br><br></p>
<hr>





<h3 id="11-16-日常"><a href="#11-16-日常" class="headerlink" title="11.16　日常"></a>11.16　日常</h3><p><b id="log">【悟】</b></p>
<ul>
<li>如何提高对自然语言处理的兴趣，让自己达到一种痴迷的地步，可以类比于看自己感兴趣的小说有时候能够不想睡觉地看，找到兴趣点；</li>
<li>选择是自己的事，可以用20分钟来打一把王者荣耀，也可以用20分钟来学习弱对偶性，今天的选择决定了明天的高度；</li>
<li>把自然语言处理技术作为主线来抓，而不是数学或者是机器学习，在实际的技术探索过程中，应用学到的数学知识或者机器学习知识并不是很多，只占了一小部分，更多的是思考如何改进或者创新，数学和机器学习只应该作为辅线来抓，它们的作用主体现在以下方面：<ul>
<li>理解模型理论方法；</li>
<li>启发改善或者创新思路；</li>
<li>为改善或者创新提供理论依据；</li>
</ul>
</li>
</ul>
<p><br><br><br></p>
<hr>





<h3 id="11-18-BERT学习"><a href="#11-18-BERT学习" class="headerlink" title="11.18　BERT学习"></a>11.18　BERT学习</h3><p><b id="log">【行】</b></p>
<ul>
<li>Transformer的学习</li>
</ul>
<p><b id="log">【悟】</b></p>
<ul>
<li>学习一个知识点就争取一次性把它学好，学彻底，不要再想着以后再来深入；</li>
<li>并不是要学的内容太多，而是没有行动，执行力是个问题；</li>
<li>学习某系比较复杂的知识点（由多个知识点组成的）时，解决一个知识点就记录一个知识点的来源位置以便于最后做总的梳理，但是不要在学习这个知识点之后就开始记录，要等到整个复杂知识点学习完毕之后，因为这样可以最有效地学习这个复杂的知识点避免自己开始就花大量的时间在局部知识点的记录上而淡忘了前面所学的知识点从而不能够很好的整体把握；</li>
<li>了解模型的缘由非常重要，从One-hot到Word2Vec，再到Elmo，再到GPT最后到BERT，每一个模型的产生都是为了克服现有模型的缺点而产生，每一次的缺点都有不同，同时也可以积累处理这些缺点的方法；</li>
</ul>
<p><br><br><br></p>
<hr>





<h3 id="11-19-BERT学习"><a href="#11-19-BERT学习" class="headerlink" title="11.19　BERT学习"></a>11.19　BERT学习</h3><p><b id="log">【行】</b></p>
<ul>
<li>Transformer的学习</li>
<li>Normalization的学习</li>
</ul>
<p><br><br><br></p>
<hr>





<h3 id="11-23-BERT学习"><a href="#11-23-BERT学习" class="headerlink" title="11.23　BERT学习"></a>11.23　BERT学习</h3><p><b id="log">【悟】</b></p>
<ul>
<li>遇到某些知识点不懂的时候，以往的查询就是直接google，然后看查到的博文，对于博文参考的文献通常视而不见，其实参考的文献很有可能才是关键内容，因为它包含原有的思想和初衷，而引用者通常水平不高，语言描述也不够深刻，所以导致了思想的传递出现偏差，所以对于不懂的知识点或者模型，我们应该尽量找原始论文或者阅读量高的解释性博文；</li>
<li><b>在学习过程中遇到有不懂的算法或者模型，不应该浅尝辄止，应该刨根问底、追根溯源，这样才能真正学得深学得扎实</b>。比如在BERT的学习过程中，对于BERT的训练方法Mask Language Model，了解的深度仅限于知道如何操作，但是至于为什么要这样操作，这样的原理是什么，并没有深究，直到后面看到有关BERT和其他预训练模型对比的博文，知道有预训练模型有自回归和自编码两种类型，自回归好理解，但是对于自编码，不是很理解，然后就开始查阅资料，了解自回归的基本概念和作用，知道自编码有几种变种，其中一种就是DAE（降噪自编码器），而这个DAE正是Mask Language Model的根本思想，这也就解释了它在BERT的训练中为什么可以这么做，所以遇到问题，多问为什么，多查资料，查原始资料才是最好的学习路径；另一个例子是关于Word2Vec的负采样学习方法，关于这个方法，在当时自己学完了也并没有理解根本原理，直到后来再次学习，追根溯源查到了它的根本原理是噪音对比估计，所以深究非常重要，同时这两个例子也告诉我，<b>应该广泛的涉猎，不应该局限于NLP这一个领域，广泛的涉猎也许能为自己在NLP的创新带来灵感和解决问题的启示</b>；</li>
<li>在查找资料，学习资料的过程中基本上都是阅读理解为主（被动思考，很少主动探索模型的产生和改进方式）；</li>
<li>学习一种模型或者算法，应该从纵向和横向两个方向进行掌握：<ul>
<li>纵向：<ul>
<li>掌握所有关于模型本身的细节和涉及的知识点；</li>
<li>探索算法的优点和缺点；</li>
</ul>
</li>
<li>横向：<ul>
<li>理清模型的发展脉络和轨迹，掌握各种历史模型的优缺点及改进思路；</li>
<li>比较各种具有同样功能的模型各自的优缺点；</li>
<li>掌握模型的变种及其变种思路；</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><br><br><br></p>
<hr>




<h2 id="2019年12月"><a href="#2019年12月" class="headerlink" title="2019年12月"></a>2019年12月</h2><h3 id="12-02-日常"><a href="#12-02-日常" class="headerlink" title="12.02　日常"></a>12.02　日常</h3><p><b id="log">【悟】</b></p>
<ul>
<li>学会在学习过程中记录问题，分辨知识点，记录关键，整合零散类，分类资料；</li>
<li>起步较晚，在有限的时间里把握更核心的内容而不是可有可无锦上添花的内容才能赶上别人；</li>
<li>利用间隙时间活动腰和脖子；</li>
<li>学会阅读源码；</li>
<li>数据的形式能够帮助阅读源码；</li>
<li>英语学习需要有指导（中英对照的），因为很多常用结构不知道该怎么翻译；</li>
<li>要逐渐形成自己的一套固定的学习方法，并在后续的学习过程中持续迭代和微调；</li>
<li>避免重复学习，浪费时间；</li>
</ul>
<p><br><br><br></p>
<hr>




<h3 id="12-15-日常"><a href="#12-15-日常" class="headerlink" title="12.15　日常"></a>12.15　日常</h3><p><b id="log">【悟】</b></p>
<ul>
<li>强化学习更类似于人的学习行为，这个框架应该提早接触；</li>
<li>即使没有大量时间仔细系统学习某个框架，比如GAN，也可以花少量时间进行简单学习了解；</li>
</ul>
<p><br><br><br></p>
<hr>




        
      </div>
      
      
      
    </div>
    


  
  
  <ul class="breadcrumb">
    
      
      
        
          <li><a href="/Menu-Log/">MENU-LOG</a></li>
          
            
          
        
      
    
      
      
        
          <li>2019</li>
        
      
    
      
      
    
  </ul>


    
    
    
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
     <i class="fa fa-drupal"></i>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.gif"
                alt="0 丨 1" />
            
              <p class="site-author-name" itemprop="name">0 丨 1</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">56</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/Menu-Category/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">19</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/Menu-Tag/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">39</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://lego4language.gitee.io/" target="_blank" title="‹‹ 数丨语 ››">‹‹ 数丨语 ››</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://lego4language.gitee.io/" target="_blank" title="简 单，">简 单，</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://lego4language.gitee.io/" target="_blank" title="执 拗，">执 拗，</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://lego4language.gitee.io/" target="_blank" title="一 直 到 老 。">一 直 到 老 。</a>
                  
                </span>
              
            </div>
          




          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-apple"></i>
                《 滴丨答 》
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.bilibili.com/video/BV1N741177BJ?p=2" title="徒手攀岩" target="_blank">徒手攀岩</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.bilibili.com/video/av4703308?from=search&seid=2865836903687886974" title="四弦人生" target="_blank">四弦人生</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.bilibili.com/bangumi/play/ep333948?theme=movie%3Ffrom%3Dsearch&seid=16732134441560039998" title="知无涯者" target="_blank">知无涯者</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.bilibili.com/bangumi/play/ep340751?theme=movie%3Ffrom%3Dsearch&seid=960190305093062102" title="澄沙之味" target="_blank">澄沙之味</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.bilibili.com/video/av33429330?from=search&seid=15360553913887371012" title="发现的乐趣" target="_blank">发现的乐趣</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="" title="有熊谷守一在的地方" target="_blank">有熊谷守一在的地方</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#2019年3月"><span class="nav-text">2019年3月</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-17-链表-t-词嵌入-t"><span class="nav-text">3.17　链表(t) + 词嵌入(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-18-链表-c-词嵌入-t"><span class="nav-text">3.18　链表(c) + 词嵌入(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-19-链表-c-词嵌入-c"><span class="nav-text">3.19　链表(c) + 词嵌入(c)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-20-链表-c-词嵌入-c"><span class="nav-text">3.20　链表(c) + 词嵌入(c)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#—-3-21-阶段性小结-—"><span class="nav-text">—　3.21　阶段性小结　—</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-22-栈-t-循环序列模型-t"><span class="nav-text">3.22　栈(t) + 循环序列模型(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-23-栈-c-RNN和LSTM-t"><span class="nav-text">3.23　栈(c) + RNN和LSTM(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-24-队列-t-seq2seq和注意力机制-t"><span class="nav-text">3.24　队列(t) + seq2seq和注意力机制(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-25-栈-c-队列-c-注意力模型-t"><span class="nav-text">3.25　栈(c) + 队列(c) + 注意力模型(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#—-3-26-短期计划-—"><span class="nav-text">—　3.26　短期计划　—</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-26-栈和队列-c-深度学习-t-Python-t"><span class="nav-text">3.26　栈和队列(c) + 深度学习(t) + Python(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-27-刷题-c-深度学习-t-机器学习-t"><span class="nav-text">3.27　刷题(c) + 深度学习(t) + 机器学习(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-28-Python-t-二叉树-t-EM算法-t-刷题-c-深度学习-t"><span class="nav-text">3.28　Python(t) + 二叉树(t) + EM算法(t) + 刷题(c) + 深度学习(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-29-EM算法-t-刷题-c"><span class="nav-text">3.29　EM算法(t) + 刷题(c)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-30-HMM-t"><span class="nav-text">3.30　HMM(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-31-深度学习-t"><span class="nav-text">3.31　深度学习(t)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019年4月"><span class="nav-text">2019年4月</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#—-4-01-短期计划-—"><span class="nav-text">—　4.01　短期计划　—</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-01-最大熵模型-t-方向导数和梯度-t"><span class="nav-text">4.01　最大熵模型(t) + 方向导数和梯度(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-02-拉格朗日乘子法与对偶性-t"><span class="nav-text">4.02　拉格朗日乘子法与对偶性(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-03-深度学习-t-PyTorch-c"><span class="nav-text">4.03　深度学习(t) + PyTorch(c)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-04-修改短期计划-深度学习-t-PyTorch-c-《自然语言处理综论》-t"><span class="nav-text">4.04　修改短期计划 + 深度学习(t) + PyTorch(c) + 《自然语言处理综论》(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-05-字典-t-刷题-c-CRF-t"><span class="nav-text">4.05　字典(t) + 刷题(c) + CRF(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-06-散列表-t-刷题-c-CRF-MaxEnt复习-t"><span class="nav-text">4.06　散列表(t) + 刷题(c) + CRF/MaxEnt复习(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-07-集合-t-字符串-t-朴素贝叶斯-t"><span class="nav-text">4.07　集合(t) + 字符串(t) + 朴素贝叶斯(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-08-字符串-t-刷题-c-SVM-t"><span class="nav-text">4.08　字符串(t) + 刷题(c) + SVM(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-09-刷题-c-Pytorch-t-依存句法分析-t"><span class="nav-text">4.09　刷题(c) + Pytorch(t) + 依存句法分析(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-10-刷题-c-Pytorch-c"><span class="nav-text">4.10　刷题(c) + Pytorch(c)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-11-应聘计划"><span class="nav-text">4.11　应聘计划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-12-刷题-c-信息抽取-t"><span class="nav-text">4.12　刷题(c) + 信息抽取(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-13-刷题-c-信息抽取及其NLP他应用了解-t"><span class="nav-text">4.13　刷题(c) + 信息抽取及其NLP他应用了解(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-15-刷题-c-Pytorch学习-t"><span class="nav-text">4.15　刷题(c) + Pytorch学习(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-16-应聘计划-修改"><span class="nav-text">4.16　应聘计划(修改)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-22-写给快28岁的自己"><span class="nav-text">4.22　写给快28岁的自己</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-23-刷题-c-词嵌入复习和gensim使用-c"><span class="nav-text">4.23　刷题(c) + 词嵌入复习和gensim使用(c)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-24-刷题-c-词嵌入复习和gensim使用-c"><span class="nav-text">4.24　刷题(c) + 词嵌入复习和gensim使用(c)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019年5月"><span class="nav-text">2019年5月</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-01-刷题-c"><span class="nav-text">5.01　刷题(c)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#—-5-03-找工作计划-—"><span class="nav-text">—　5.03　找工作计划　—</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-07-刷题-c-深度学习复习-t"><span class="nav-text">5.07　刷题(c) + 深度学习复习(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-08-pytorch熟悉使用-c"><span class="nav-text">5.08　pytorch熟悉使用(c)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-09-pytorch熟悉使用-c"><span class="nav-text">5.09　pytorch熟悉使用(c)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-10-刷题-c"><span class="nav-text">5.10　刷题(c)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-12-随机思索"><span class="nav-text">5.12　随机思索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-14-刷题-c-MaxEnt复习-t-KNN复习和实现-c"><span class="nav-text">5.14　刷题(c) + MaxEnt复习(t) + KNN复习和实现(c)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-15-刷题-c-HMM、CRF、MaxEnt复习-t-决策树复习-t"><span class="nav-text">5.15　刷题(c) + HMM、CRF、MaxEnt复习(t) + 决策树复习(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-16-复习刷过的题-c-Pytorch使用建模-c-AdaBoost复习-t"><span class="nav-text">5.16　复习刷过的题(c) + Pytorch使用建模(c) + AdaBoost复习(t)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-17-复习刷过的题-c-Pytorch使用建模-c"><span class="nav-text">5.17　复习刷过的题(c) + Pytorch使用建模(c)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-18-2017-2019-ACL各子领域论文统计"><span class="nav-text">5.18　2017-2019 ACL各子领域论文统计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-23-休息"><span class="nav-text">5.23　休息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-27-Think-of-NLP-as-a-Life"><span class="nav-text">5.27　Think of NLP as a Life . . .</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-28-刷题"><span class="nav-text">5.28　刷题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-30-找工作计划"><span class="nav-text">5.30　找工作计划</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019年6月"><span class="nav-text">2019年6月</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-25-反思状态、制定计划"><span class="nav-text">6.25　反思状态、制定计划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-26-刷题-分词项目-概率学习"><span class="nav-text">6.26　刷题 + 分词项目 + 概率学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-27-刷题-感知机学习-概率学习"><span class="nav-text">6.27　刷题 + 感知机学习 + 概率学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-28-刷题-感知机学习-复习中文分词系统"><span class="nav-text">6.28　刷题 + 感知机学习 + 复习中文分词系统</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-29-休整"><span class="nav-text">6.29　休整</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-30-学习《机率》"><span class="nav-text">6.30　学习《机率》</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019年7月"><span class="nav-text">2019年7月</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-01-学习《机率》"><span class="nav-text">7.01　学习《机率》</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-02-学完《机率》"><span class="nav-text">7.02　学完《机率》</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-03-刷题-《统计学习方法》复习"><span class="nav-text">7.03　刷题 + 《统计学习方法》复习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-04-刷题-《统计学习方法》复习"><span class="nav-text">7.04　刷题 + 《统计学习方法》复习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-05-刷题-deeplearning复习"><span class="nav-text">7.05　刷题 + deeplearning复习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-06-休整-刷题-deeplearning复习"><span class="nav-text">7.06　休整 + 刷题 + deeplearning复习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-07-休整-线代-deeplearning复习"><span class="nav-text">7.07　休整 + 线代 + deeplearning复习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-08-线代-deeplearning复习"><span class="nav-text">7.08　线代 + deeplearning复习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-09-休整-线代"><span class="nav-text">7.09　休整 + 线代</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-10-制定学习计划"><span class="nav-text">7.10　制定学习计划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-15-Nothing"><span class="nav-text">7.15　Nothing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-16-lt-再见，过去-gt"><span class="nav-text">7.16　&lt; 再见，过去 &gt;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-17-刷题-Pytorch复习"><span class="nav-text">7.17　刷题 + Pytorch复习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-18-刷题-Pytorch中LSTM学习-词性标注"><span class="nav-text">7.18　刷题 + Pytorch中LSTM学习 + 词性标注</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-19-刷题-Pytorch中LSTM学习-词性标注"><span class="nav-text">7.19　刷题 + Pytorch中LSTM学习 + 词性标注</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-20-刷题-Pytorch中LSTM学习-深度学习复习"><span class="nav-text">7.20　刷题 + Pytorch中LSTM学习 + 深度学习复习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-21-《Python3学习笔记》-LSTM词性标记总结"><span class="nav-text">7.21　《Python3学习笔记》 + LSTM词性标记总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-22-《Python3学习笔记》-LSTM词性标记总结"><span class="nav-text">7.22　《Python3学习笔记》 + LSTM词性标记总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-23-刷题-CRF、HMM、Maxent"><span class="nav-text">7.23　刷题 + CRF、HMM、Maxent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-24-刷题-最大熵模型数学公式推导-IIS算法"><span class="nav-text">7.24　刷题 + 最大熵模型数学公式推导 + IIS算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-25-刷题"><span class="nav-text">7.25　刷题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-26-EM、HMM、MaxEnt、MEMM、CRF-通览"><span class="nav-text">7.26　EM、HMM、MaxEnt、MEMM、CRF 通览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-27-句法级复习"><span class="nav-text">7.27　句法级复习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-28-休息"><span class="nav-text">7.28　休息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-29-哈希结构-N元语法-句法"><span class="nav-text">7.29　哈希结构 + N元语法 + 句法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-30-Pytorch官网中LSTM-CRF作NER"><span class="nav-text">7.30　Pytorch官网中LSTM+CRF作NER</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-31-依存句法-感知机扩展-HMM估计参数复习"><span class="nav-text">7.31　依存句法 + 感知机扩展 + HMM估计参数复习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019年8月"><span class="nav-text">2019年8月</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-01-基于结构化平均感知机的标注器"><span class="nav-text">8.01　基于结构化平均感知机的标注器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-08-28岁，启程"><span class="nav-text">8.08　28岁，启程 . . .</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#0-Tips"><span class="nav-text">0. Tips</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-编程类"><span class="nav-text">1. 编程类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-自然语言处理"><span class="nav-text">2. 自然语言处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-机器学习"><span class="nav-text">3. 机器学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-深度学习"><span class="nav-text">4. 深度学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-应聘"><span class="nav-text">5. 应聘</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-09-动态规划-文本分类"><span class="nav-text">8.09　动态规划 + 文本分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-11-CS224N第01讲"><span class="nav-text">8.11　CS224N第01讲</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-12-CS224N第02、03讲"><span class="nav-text">8.12　CS224N第02、03讲</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-14-CBOW-Hierarchecal-Softmax-实现词向量"><span class="nav-text">8.14　CBOW + Hierarchecal Softmax 实现词向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-15-CBOW-Negative-Sampling-实现词向量"><span class="nav-text">8.15　CBOW + Negative Sampling 实现词向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-21-词向量项目总结"><span class="nav-text">8.21　词向量项目总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-28-建站"><span class="nav-text">8.28　建站</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019年9月"><span class="nav-text">2019年9月</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-02-计划"><span class="nav-text">9.02　计划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-06-Do-Nothing"><span class="nav-text">9.06　Do Nothing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-15-计划"><span class="nav-text">9.15　计划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-16-学习CS224N（2017版）"><span class="nav-text">9.16　学习CS224N（2017版）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-17-学习CS224N（2017版）"><span class="nav-text">9.17　学习CS224N（2017版）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-18-学习CS224N（2017版）"><span class="nav-text">9.18　学习CS224N（2017版）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019年10月"><span class="nav-text">2019年10月</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-01-学习"><span class="nav-text">10.01 学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-16-找工作复习实践计划"><span class="nav-text">10.16　找工作复习实践计划</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-工程"><span class="nav-text">1. 工程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-理论"><span class="nav-text">2. 理论</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019年11月"><span class="nav-text">2019年11月</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#11-14-日常"><span class="nav-text">11.14　日常</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-16-日常"><span class="nav-text">11.16　日常</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-18-BERT学习"><span class="nav-text">11.18　BERT学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-19-BERT学习"><span class="nav-text">11.19　BERT学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-23-BERT学习"><span class="nav-text">11.23　BERT学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019年12月"><span class="nav-text">2019年12月</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#12-02-日常"><span class="nav-text">12.02　日常</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-15-日常"><span class="nav-text">12.15　日常</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <!--<div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love" id="animate">
    <i class="fa fa-drupal"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">0 丨 1</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="站点总字数">352k</span>
  

  
</div>
-->

<!--
-->

<!--




  <div class="theme-info">主题 – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Mist</a> v6.4.1</div>
-->

<img class='footerpicture' src="/images/footer.png">

<!--
-->

        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  















  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.1"></script>



  



  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  

  <!-- 代码块复制功能 -->
  <script type="text/javascript" src="/js/src/clipboard.min.js"></script>
  <script type="text/javascript" src="/js/src/clipboard-use.js"></script>
</body>
</html>
